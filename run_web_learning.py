# /run_web_learning.py
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: matsushibadenki/snn3/SNN3-176e5ceb739db651438b22d74c0021f222858011/run_web_learning.py
# ã‚¿ã‚¤ãƒˆãƒ«: Autonomous Web Learning Script
# æ©Ÿèƒ½èª¬æ˜: DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹éš›ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã—ã€
#            Optimizerã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ­£ã—ãæ¸¡ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã€TypeErrorã‚’è§£æ¶ˆã™ã‚‹ã€‚

import argparse
import os
import asyncio
from snn_research.tools.web_crawler import WebCrawler
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from app.containers import TrainingContainer # DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆ©ç”¨

def main():
    """
    Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¨ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é€£æºã•ã›ã€
    æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå¾‹çš„ã«ç”Ÿæˆã™ã‚‹ã€‚
    """
    parser = argparse.ArgumentParser(
        description="Autonomous Web Learning Framework",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--topic",
        type=str,
        required=True,
        help="å­¦ç¿’ã•ã›ãŸã„ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚¿ã‚¹ã‚¯åã¨ã—ã¦ä½¿ç”¨ï¼‰ã€‚\nä¾‹: 'æœ€æ–°ã®AIæŠ€è¡“'"
    )
    parser.add_argument(
        "--start_url",
        type=str,
        required=True,
        help="æƒ…å ±åé›†ã‚’é–‹å§‹ã™ã‚‹èµ·ç‚¹ã¨ãªã‚‹URLã€‚\nä¾‹: 'https://www.itmedia.co.jp/news/subtop/aiplus/'"
    )
    parser.add_argument(
        "--max_pages",
        type=int,
        default=5, # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªãè¨­å®š
        help="åé›†ã™ã‚‹Webãƒšãƒ¼ã‚¸ã®æœ€å¤§æ•°ã€‚"
    )

    args = parser.parse_args()

    # --- ã‚¹ãƒ†ãƒƒãƒ—1: Webã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åé›† ---
    print("\n" + "="*20 + " ğŸŒ Step 1: Web Crawling " + "="*20)
    crawler = WebCrawler()
    crawled_data_path = crawler.crawl(start_url=args.start_url, max_pages=args.max_pages)

    if not os.path.exists(crawled_data_path) or os.path.getsize(crawled_data_path) == 0:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã§ããªã‹ã£ãŸãŸã‚ã€å­¦ç¿’ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        return

    # --- ã‚¹ãƒ†ãƒƒãƒ—2: ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰çŸ¥è­˜è’¸ç•™ã«ã‚ˆã‚‹å­¦ç¿’ ---
    print("\n" + "="*20 + " ğŸ§  Step 2: On-demand Learning " + "="*20)
    
    container = TrainingContainer()
    container.config.from_yaml("configs/base_config.yaml")
    container.config.from_yaml("configs/models/small.yaml")

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    # ä¾å­˜é–¢ä¿‚ã‚’æ­£ã—ã„é †åºã§æ§‹ç¯‰ã™ã‚‹
    # 1. ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    student_model = container.snn_model()

    # 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    optimizer = container.optimizer(params=student_model.parameters())

    # 3. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ä½¿ã£ã¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    scheduler = container.scheduler(optimizer=optimizer)

    # 4. æ§‹ç¯‰ã—ãŸä¾å­˜é–¢ä¿‚ã‚’æ¸¡ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    distillation_trainer = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
    )

    distillation_manager = KnowledgeDistillationManager(
        student_model=student_model,
        trainer=distillation_trainer,
        teacher_model_name=container.config.training.gradient_based.distillation.teacher_model(),
        tokenizer_name=container.config.data.tokenizer_name(),
        model_registry=container.model_registry(),
        device=container.device()
    )
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    asyncio.run(distillation_manager.run_on_demand_pipeline(
        task_description=args.topic,
        unlabeled_data_path=crawled_data_path,
        force_retrain=True
    ))

    print("\nğŸ‰ è‡ªå¾‹çš„ãªWebå­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    print(f"  ãƒˆãƒ”ãƒƒã‚¯ã€Œ{args.topic}ã€ã«é–¢ã™ã‚‹æ–°ã—ã„å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ãŒè‚²æˆã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
