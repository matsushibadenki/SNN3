# matsushibadenki/snn2/train.py
# (æ—§ snn_research/training/main.py)
#
# æ–°ã—ã„çµ±åˆå­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ (å®Œå…¨ç‰ˆ)
#
# æ©Ÿèƒ½:
# - DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã—ã€å­¦ç¿’ã«å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å‹•çš„ã«çµ„ã¿ç«‹ã¦ã‚‹ã€‚
# - --override_config å¼•æ•°ã§ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ä»»æ„ã®è¨­å®šã‚’ä¸Šæ›¸ãå¯èƒ½ã€‚
# - åˆ†æ•£å­¦ç¿’ (`--distributed`) ã«å¯¾å¿œã€‚
# - å‹¾é…ãƒ™ãƒ¼ã‚¹å­¦ç¿’ã¨ç”Ÿç‰©å­¦çš„å­¦ç¿’ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’configãƒ•ã‚¡ã‚¤ãƒ«ã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã€‚
# - æ—¢å­˜ã®æ©Ÿèƒ½ã‚’ã™ã¹ã¦ç¶­æŒã—ã€çœç•¥ã—ãªã„å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã€‚
# - å¤‰æ›´ç‚¹: ä¸è¦ã«ãªã£ãŸå¤ã„ç”Ÿç‰©å­¦çš„å­¦ç¿’(BioTrainer)ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤ã€‚

import argparse
import os
import torch
import torch.distributed as dist
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, random_split, DistributedSampler
from dependency_injector.wiring import inject, Provide
from typing import Optional, Tuple, List, Dict, Any, Callable

from app.containers import TrainingContainer
from snn_research.data.datasets import get_dataset_class, DistillationDataset, DataFormat, SNNBaseDataset
from snn_research.training.trainers import BreakthroughTrainer


# DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
container = TrainingContainer()

@inject
def train(
    args,
    config: Dict[str, Any] = Provide[TrainingContainer.config],
    tokenizer=Provide[TrainingContainer.tokenizer],
):
    """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    is_distributed = args.distributed
    rank = int(os.environ.get("LOCAL_RANK", -1))
    device = f'cuda:{rank}' if is_distributed and torch.cuda.is_available() else get_auto_device()
    
    paradigm = config['training']['paradigm']
    is_distillation = paradigm == "gradient_based" and config['training']['gradient_based']['type'] == "distillation"
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã‘ã‚Œã°configã®å€¤ã‚’ä½¿ç”¨
    data_path = args.data_path or config['data']['path']
    
    DatasetClass = get_dataset_class(DataFormat(config['data']['format']))
    dataset = DistillationDataset(
        file_path=os.path.join(data_path, "distillation_data.jsonl"), data_dir=data_path,
        tokenizer=tokenizer, max_seq_len=config['model']['time_steps']
    ) if is_distillation else DatasetClass(
        file_path=data_path, tokenizer=tokenizer, max_seq_len=config['model']['time_steps']
    )
        
    train_size = int((1.0 - config['data']['split_ratio']) * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_sampler = DistributedSampler(train_dataset) if is_distributed else None
    train_loader = DataLoader(
        train_dataset, batch_size=config['training']['batch_size'], shuffle=(train_sampler is None),
        sampler=train_sampler, collate_fn=collate_fn(tokenizer, is_distillation)
    )
    val_loader = DataLoader(
        val_dataset, batch_size=config['training']['batch_size'], shuffle=False,
        collate_fn=collate_fn(tokenizer, is_distillation)
    )

    print(f"ğŸš€ å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  '{paradigm}' ã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")

    if paradigm in ["gradient_based", "self_supervised", "physics_informed"]:
        if is_distributed and paradigm != "gradient_based":
            raise NotImplementedError(f"{paradigm} learning does not support DDP yet.")
        
        snn_model = container.snn_model().to(device)
        if is_distributed:
            snn_model = DDP(snn_model, device_ids=[rank], find_unused_parameters=True)
        
        astrocyte = container.astrocyte_network(snn_model=snn_model) if args.use_astrocyte else None
        
        trainer: BreakthroughTrainer
        if paradigm == "gradient_based":
            optimizer = container.optimizer(params=snn_model.parameters())
            scheduler = container.scheduler(optimizer=optimizer) if config['training']['gradient_based']['use_scheduler'] else None
            trainer_provider = container.distillation_trainer if is_distillation else container.standard_trainer
            trainer = trainer_provider(model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte)
        elif paradigm == "self_supervised":
            optimizer = container.ssl_optimizer(params=snn_model.parameters())
            scheduler = container.ssl_scheduler(optimizer=optimizer) if config['training']['self_supervised']['use_scheduler'] else None
            trainer = container.self_supervised_trainer(model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte)
        else: # physics_informed
            optimizer = container.pi_optimizer(params=snn_model.parameters())
            scheduler = container.pi_scheduler(optimizer=optimizer) if config['training']['physics_informed']['use_scheduler'] else None
            trainer = container.physics_informed_trainer(model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte)
        
        start_epoch = trainer.load_checkpoint(args.resume_path) if args.resume_path and paradigm == "gradient_based" else 0
        for epoch in range(start_epoch, config['training']['epochs']):
            if train_sampler: train_sampler.set_epoch(epoch)
            trainer.train_epoch(train_loader, epoch)
            if rank in [-1, 0] and (epoch % config['training']['eval_interval'] == 0 or epoch == config['training']['epochs'] - 1):
                val_metrics = trainer.evaluate(val_loader, epoch)
                if paradigm == "gradient_based" and epoch % config['training']['log_interval'] == 0:
                    checkpoint_path = os.path.join(config['training']['log_dir'], f"checkpoint_epoch_{epoch}.pth")
                    trainer.save_checkpoint(
                        path=checkpoint_path, epoch=epoch, metric_value=val_metrics.get('total', float('inf')),
                        tokenizer_name=config['data']['tokenizer_name'], config=config['model']
                    )

    else:
        raise ValueError(f"Unknown or unsupported training paradigm for this script: '{paradigm}'. Use run_rl_agent.py for 'biologically_plausible'.")

    if rank in [-1, 0]: print("âœ… å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


def collate_fn(tokenizer, is_distillation: bool) -> Callable[[List[Tuple[torch.Tensor, ...]]], Tuple[torch.Tensor, ...]]:
    def collate(batch: List[Tuple[torch.Tensor, ...]]) -> Tuple[torch.Tensor, ...]:
        inputs = [item[0] for item in batch]
        targets = [item[1] for item in batch]
        padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)
        padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)
        if is_distillation:
            logits = [item[2] for item in batch]
            padded_logits = torch.nn.utils.rnn.pad_sequence(logits, batch_first=True, padding_value=0.0)
            return padded_inputs, padded_targets, padded_logits
        return padded_inputs, padded_targets
    return collate
    
def get_auto_device() -> str:
    """å®Ÿè¡Œç’°å¢ƒã«æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•çš„ã«é¸æŠã™ã‚‹ã€‚"""
    if torch.cuda.is_available(): return "cuda"
    if torch.backends.mps.is_available(): return "mps"
    return "cpu"
    
def main():
    parser = argparse.ArgumentParser(description="SNN çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--model_config", type=str, help="ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--data_path", type=str, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ï¼ˆconfigã‚’ä¸Šæ›¸ãï¼‰")
    parser.add_argument("--override_config", type=str, action='append', help="è¨­å®šã‚’ä¸Šæ›¸ã (ä¾‹: 'training.epochs=5')")
    parser.add_argument("--distributed", action="store_true", help="åˆ†æ•£å­¦ç¿’ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--resume_path", type=str, help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã™ã‚‹ (gradient_basedã®ã¿)")
    parser.add_argument("--use_astrocyte", action="store_true", help="ã‚¢ã‚¹ãƒˆãƒ­ã‚µã‚¤ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æœ‰åŠ¹ã«ã™ã‚‹ (gradient_basedç³»ã®ã¿)")
    args = parser.parse_args()

    container.config.from_yaml(args.config)
    if args.model_config: container.config.from_yaml(args.model_config)
    
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    if args.override_config:
        for override in args.override_config:
            key, value = override.split('=', 1)
            # ãƒ‰ãƒƒãƒˆè¨˜æ³•ã®ã‚­ãƒ¼ã‚’ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã«å¤‰æ›ã—ã¦ãƒãƒ¼ã‚¸ã™ã‚‹
            keys = key.split('.')
            d = {}
            ref = d
            for k in keys[:-1]:
                ref[k] = {}
                ref = ref[k]
            
            # å€¤ã‚’é©åˆ‡ãªå‹ã«å¤‰æ›ã—ã‚ˆã†ã¨è©¦ã¿ã‚‹ (ä¾‹: "true" -> True)
            if value.lower() == 'true':
                ref[keys[-1]] = True
            elif value.lower() == 'false':
                ref[keys[-1]] = False
            else:
                try:
                    # æ•´æ•°ã‚„æµ®å‹•å°æ•°ç‚¹æ•°ã«å¤‰æ›
                    ref[keys[-1]] = int(value)
                except ValueError:
                    try:
                        ref[keys[-1]] = float(value)
                    except ValueError:
                        # å¤‰æ›ã§ããªã„å ´åˆã¯æ–‡å­—åˆ—ã®ã¾ã¾
                        ref[keys[-1]] = value
            
            container.config.from_dict(d)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            
    if args.distributed: dist.init_process_group(backend="nccl")
    
    # DIã‚³ãƒ³ãƒ†ãƒŠã®wiring: mainé–¢æ•°å†…ã§è¡Œã†ã“ã¨ã§ã€è¨­å®šèª­ã¿è¾¼ã¿å¾Œã«ä¾å­˜é–¢ä¿‚ã‚’è§£æ±º
    container.wire(modules=[__name__])
    
    # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ³¨å…¥ã•ã‚ŒãŸconfigã¯dictã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹ãŸã‚ã€ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ã‚’å¤‰æ›´
    # @injectãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãŒé©ç”¨ã•ã‚ŒãŸé–¢æ•°å†…ã§ã¯ã€configã¯é€šå¸¸ã®è¾æ›¸ã¨ã—ã¦æŒ¯ã‚‹èˆã†
    injected_config = container.config()
    injected_tokenizer = container.tokenizer()
    train(args, config=injected_config, tokenizer=injected_tokenizer)
    
    if args.distributed: dist.destroy_process_group()

if __name__ == "__main__":
    main()
