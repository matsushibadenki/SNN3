# /snn-cli.py
# Title: çµ±åˆCLIãƒ„ãƒ¼ãƒ«
# Description: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨æ©Ÿèƒ½ã‚’ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚

import argparse
import sys
import os
from pathlib import Path

# --- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ  ---
sys.path.append(str(Path(__file__).resolve().parent))


# --- å„æ©Ÿèƒ½ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from snn_research.agent import AutonomousAgent, SelfEvolvingAgent, DigitalLifeForm, ReinforcementLearnerAgent
from snn_research.cognitive_architecture import HierarchicalPlanner

from snn_research.rl_env.simple_env import SimpleEnvironment
import train as gradient_based_trainer # train.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

def handle_agent(args):
    """è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    agent = AutonomousAgent()
    selected_model_info = agent.handle_task(
        task_description=args.task,
        unlabeled_data_path=args.unlabeled_data_path,
        force_retrain=args.force_retrain
    )
    if selected_model_info and args.prompt:
        print("\n" + "="*20 + " ğŸ§  INFERENCE " + "="*20)
        print(f"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        agent.run_inference(selected_model_info, args.prompt)
    elif not selected_model_info:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)
        print("ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

def handle_planner(args):
    """éšå±¤çš„ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    planner = HierarchicalPlanner()
    final_result = planner.execute_task(
        task_request=args.request,
        context=args.context
    )
    if final_result:
        print("\n" + "="*20 + " âœ… FINAL RESULT " + "="*20)
        print(final_result)
    else:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)

def handle_life_form(args):
    """ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    life_form = DigitalLifeForm(project_root=".")
    life_form.awareness_loop(cycles=args.cycles)

def handle_evolution(args):
    """è‡ªå·±é€²åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    agent = SelfEvolvingAgent(project_root=".", model_config_path=args.model_config)
    initial_metrics = {
        "accuracy": args.initial_accuracy,
        "avg_spikes_per_sample": args.initial_spikes
    }
    agent.run_evolution_cycle(
        task_description=args.task_description,
        initial_metrics=initial_metrics
    )

def handle_rl(args):
    """ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    # run_rl_agent.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«çµ±åˆ
    import torch
    from tqdm import tqdm

    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    env = SimpleEnvironment(pattern_size=args.pattern_size, device=device)
    agent = ReinforcementLearnerAgent(input_size=args.pattern_size, output_size=args.pattern_size, device=device)
    
    progress_bar = tqdm(range(args.episodes))
    total_reward = 0.0

    for episode in progress_bar:
        state = env.reset()
        action = agent.get_action(state)
        _, reward, _ = env.step(action)
        agent.learn(reward)
        total_reward += reward
        avg_reward = total_reward / (episode + 1)
        progress_bar.set_postfix({"Avg Reward": f"{avg_reward:.3f}"})
    
    print(f"\nâœ… å­¦ç¿’å®Œäº†ã€‚æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {total_reward / args.episodes:.4f}")

def handle_train(args):
    """å‹¾é…ãƒ™ãƒ¼ã‚¹å­¦ç¿’ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    # train.py ã® main() é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    # train.pyå†…ã®DIã‚³ãƒ³ãƒ†ãƒŠè¨­å®šãŒãã®ã¾ã¾åˆ©ç”¨ã•ã‚Œã‚‹
    print("ğŸ”§ å‹¾é…ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # train.pyã®mainé–¢æ•°ãŒå¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ãŸã‚ã€sys.argvã‚’ä¸€æ™‚çš„ã«æ›¸ãæ›ãˆã‚‹
    original_argv = sys.argv
    sys.argv = [original_argv[0]] + args.train_args
    gradient_based_trainer.main()
    sys.argv = original_argv # å…ƒã«æˆ»ã™

def main():
    parser = argparse.ArgumentParser(
        description="Project SNN: çµ±åˆCLIãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawTextHelpFormatter
    )
    subparsers = parser.add_subparsers(dest="command", required=True, help="å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ")

    # --- Agent Subcommand ---
    parser_agent = subparsers.add_parser("agent", help="è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ“ä½œã—ã¦å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
    parser_agent.add_argument("solve", help="æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã—ã¾ã™")
    parser_agent.add_argument("--task", type=str, required=True, help="ã‚¿ã‚¹ã‚¯ã®è‡ªç„¶è¨€èªèª¬æ˜ (ä¾‹: 'æ„Ÿæƒ…åˆ†æ')")
    parser_agent.add_argument("--prompt", type=str, help="æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser_agent.add_argument("--unlabeled_data_path", type=str, help="æ–°è¦å­¦ç¿’æ™‚ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹")
    parser_agent.add_argument("--force_retrain", action="store_true", help="ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ç°¿ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶çš„ã«å†å­¦ç¿’")
    parser_agent.set_defaults(func=handle_agent)

    # --- Planner Subcommand ---
    parser_planner = subparsers.add_parser("planner", help="é«˜æ¬¡èªçŸ¥ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’æ“ä½œã—ã¦è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
    parser_planner.add_argument("execute", help="è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¦æ±‚ã‚’å®Ÿè¡Œã—ã¾ã™")
    parser_planner.add_argument("--request", type=str, required=True, help="ã‚¿ã‚¹ã‚¯è¦æ±‚ (ä¾‹: 'è¨˜äº‹ã‚’è¦ç´„ã—ã¦æ„Ÿæƒ…ã‚’åˆ†æ')")
    parser_planner.add_argument("--context", type=str, required=True, help="å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿")
    parser_planner.set_defaults(func=handle_planner)

    # --- Life Form Subcommand ---
    parser_life = subparsers.add_parser("life-form", help="ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®è‡ªå¾‹ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹")
    parser_life.add_argument("start", help="æ„è­˜ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™")
    parser_life.add_argument("--cycles", type=int, default=5, help="å®Ÿè¡Œã™ã‚‹æ„è­˜ã‚µã‚¤ã‚¯ãƒ«ã®å›æ•°")
    parser_life.set_defaults(func=handle_life_form)

    # --- Evolution Subcommand ---
    parser_evo = subparsers.add_parser("evolve", help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ")
    parser_evo.add_argument("run", help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’1å›å®Ÿè¡Œã—ã¾ã™")
    parser_evo.add_argument("--task_description", type=str, required=True, help="è‡ªå·±è©•ä¾¡ã®èµ·ç‚¹ã¨ãªã‚‹ã‚¿ã‚¹ã‚¯èª¬æ˜")
    parser_evo.add_argument("--model_config", type=str, default="configs/models/small.yaml", help="é€²åŒ–å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser_evo.add_argument("--initial_accuracy", type=float, default=0.75, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸç²¾åº¦")
    parser_evo.add_argument("--initial_spikes", type=float, default=1500.0, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸã‚¹ãƒ‘ã‚¤ã‚¯æ•°")
    parser_evo.set_defaults(func=handle_evolution)
    
    # --- Reinforcement Learning Subcommand ---
    parser_rl = subparsers.add_parser("rl", help="ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œ")
    parser_rl.add_argument("run", help="å¼·åŒ–å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™")
    parser_rl.add_argument("--episodes", type=int, default=100, help="å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°")
    parser_rl.add_argument("--pattern_size", type=int, default=10, help="ç’°å¢ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚µã‚¤ã‚º")
    parser_rl.set_defaults(func=handle_rl)

    # --- Train Subcommand ---
    # æ®‹ã‚Šã®å¼•æ•°ã‚’ã™ã¹ã¦train.pyã«æ¸¡ã™ãŸã‚ã®ãƒ‘ãƒ¼ã‚µãƒ¼
    parser_train = subparsers.add_parser("train", help="å‹¾é…ãƒ™ãƒ¼ã‚¹ã§SNNãƒ¢ãƒ‡ãƒ«ã‚’æ‰‹å‹•å­¦ç¿’ (train.pyã®å¼•æ•°ã‚’æŒ‡å®š)")
    parser_train.add_argument('train_args', nargs=argparse.REMAINDER, help="train.pyã«æ¸¡ã™å¼•æ•° (ä¾‹: --config ...)")
    parser_train.set_defaults(func=handle_train)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
