# matsushibadenki/snn3/SNN3-5b728d05237b1a32304ee6af1a9240f1ebfe55ff/snn-cli.py
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: matsushibadenki/snn3/snn-cli.py
# ã‚¿ã‚¤ãƒˆãƒ«: çµ±åˆCLIãƒ„ãƒ¼ãƒ« (typerç‰ˆ)
# æ©Ÿèƒ½èª¬æ˜: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨æ©Ÿèƒ½ã‚’ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚
#           argparseã¨typerã®æ··åœ¨ã«ã‚ˆã£ã¦ç™ºç”Ÿã—ã¦ã„ãŸå¼•æ•°è§£æã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€
#           typerã«å®Œå…¨ã«ç§»è¡Œã€‚gradient-trainãŒè¿½åŠ ã®å¼•æ•°ã‚’æ­£ã—ã
#           train.pyã«æ¸¡ã›ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚

import sys
from pathlib import Path
import asyncio
import torch
import typer
from typing import List, Optional

# --- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ  ---
# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ­£ã—ãã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã‚‹
sys.path.append(str(Path(__file__).resolve().parent))

# --- å„æ©Ÿèƒ½ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from snn_research.agent.autonomous_agent import AutonomousAgent
from snn_research.agent.self_evolving_agent import SelfEvolvingAgent
from snn_research.agent.digital_life_form import DigitalLifeForm
from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner
from snn_research.rl_env.simple_env import SimpleEnvironment
import train as gradient_based_trainer
from snn_research.distillation.model_registry import SimpleModelRegistry
from snn_research.agent.memory import Memory
from snn_research.tools.web_crawler import WebCrawler
from snn_research.cognitive_architecture.rag_snn import RAGSystem

# --- CLIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®šç¾© ---
app = typer.Typer(
    help="Project SNN: çµ±åˆCLIãƒ„ãƒ¼ãƒ«",
    rich_markup_mode="md",
    add_completion=False
)

# --- ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ ---
agent_app = typer.Typer(help="è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ“ä½œã—ã¦å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
app.add_typer(agent_app, name="agent")

planner_app = typer.Typer(help="é«˜æ¬¡èªçŸ¥ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’æ“ä½œã—ã¦è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
app.add_typer(planner_app, name="planner")

life_form_app = typer.Typer(help="ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®è‡ªå¾‹ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹")
app.add_typer(life_form_app, name="life-form")

evolve_app = typer.Typer(help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ")
app.add_typer(evolve_app, name="evolve")

rl_app = typer.Typer(help="ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œ")
app.add_typer(rl_app, name="rl")

# --- agent ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@agent_app.command("solve", help="æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã—ã¾ã™ã€‚å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢ã€ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å­¦ç¿’ã€æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
def agent_solve(
    task: str = typer.Option(..., help="ã‚¿ã‚¹ã‚¯ã®è‡ªç„¶è¨€èªèª¬æ˜ (ä¾‹: 'æ„Ÿæƒ…åˆ†æ')"),
    prompt: Optional[str] = typer.Option(None, help="æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"),
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    unlabeled_data: Optional[Path] = typer.Option(None, help="æ–°è¦å­¦ç¿’æ™‚ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹", exists=True, file_okay=True, dir_okay=False),
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    force_retrain: bool = typer.Option(False, "--force-retrain", help="ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ç°¿ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶çš„ã«å†å­¦ç¿’"),
    min_accuracy: float = typer.Option(0.6, help="å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®æœ€ä½ç²¾åº¦è¦ä»¶"),
    max_spikes: float = typer.Option(10000.0, help="å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®å¹³å‡ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ä¸Šé™")
):
    model_registry = SimpleModelRegistry()
    rag_system = RAGSystem()
    memory = Memory()
    web_crawler = WebCrawler()
    planner = HierarchicalPlanner(model_registry=model_registry, rag_system=rag_system)

    agent = AutonomousAgent(
        name="cli-agent",
        planner=planner,
        model_registry=model_registry,
        memory=memory,
        web_crawler=web_crawler,
        accuracy_threshold=min_accuracy,
        energy_budget=max_spikes
    )
    
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    selected_model_info = asyncio.run(agent.handle_task(
        task_description=task,
        unlabeled_data_path=str(unlabeled_data) if unlabeled_data else None,
        force_retrain=force_retrain
    ))
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    
    if selected_model_info and prompt:
        print("\n" + "="*20 + " ğŸ§  INFERENCE " + "="*20)
        print(f"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        asyncio.run(agent.run_inference(selected_model_info, prompt))
    elif not selected_model_info:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)
        print("ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- planner ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@planner_app.command("execute", help="è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¦æ±‚ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å†…éƒ¨ã§è¨ˆç”»ã‚’ç«‹æ¡ˆã—ã€è¤‡æ•°ã®å°‚é–€å®¶ã‚’é€£æºã•ã›ã¾ã™ã€‚")
def planner_execute(
    request: str = typer.Option(..., help="ã‚¿ã‚¹ã‚¯è¦æ±‚ (ä¾‹: 'è¨˜äº‹ã‚’è¦ç´„ã—ã¦æ„Ÿæƒ…ã‚’åˆ†æ')"),
    context: str = typer.Option(..., help="å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿")
):
    model_registry = SimpleModelRegistry()
    rag_system = RAGSystem()
    planner = HierarchicalPlanner(model_registry=model_registry, rag_system=rag_system)
    
    final_result = planner.execute_task(task_request=request, context=context)
    if final_result:
        print("\n" + "="*20 + " âœ… FINAL RESULT " + "="*20)
        print(final_result)
    else:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)

# --- life-form ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@life_form_app.command("start", help="æ„è­˜ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™ã€‚AIãŒè‡ªå¾‹çš„ã«æ€è€ƒãƒ»å­¦ç¿’ã—ã¾ã™ã€‚")
def life_form_start(cycles: int = typer.Option(5, help="å®Ÿè¡Œã™ã‚‹æ„è­˜ã‚µã‚¤ã‚¯ãƒ«ã®å›æ•°")):
    life_form = DigitalLifeForm()
    life_form.awareness_loop(cycles=cycles)

# --- evolve ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@evolve_app.command("run", help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’1å›å®Ÿè¡Œã—ã¾ã™ã€‚AIãŒè‡ªèº«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ”¹å–„ã—ã¾ã™ã€‚")
def evolve_run(
    task_description: str = typer.Option(..., help="è‡ªå·±è©•ä¾¡ã®èµ·ç‚¹ã¨ãªã‚‹ã‚¿ã‚¹ã‚¯èª¬æ˜"),
    model_config: Path = typer.Option("configs/models/small.yaml", help="é€²åŒ–å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«", exists=True),
    initial_accuracy: float = typer.Option(0.75, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸç²¾åº¦"),
    initial_spikes: float = typer.Option(1500.0, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸã‚¹ãƒ‘ã‚¤ã‚¯æ•°")
):
    model_registry = SimpleModelRegistry()
    rag_system = RAGSystem()
    memory = Memory()
    web_crawler = WebCrawler()
    planner = HierarchicalPlanner(model_registry=model_registry, rag_system=rag_system)

    agent = SelfEvolvingAgent(
        name="evolving-agent",
        planner=planner,
        model_registry=model_registry,
        memory=memory,
        web_crawler=web_crawler,
        project_root=".",
        model_config_path=str(model_config)
    )
    initial_metrics = {
        "accuracy": initial_accuracy,
        "avg_spikes_per_sample": initial_spikes
    }
    agent.run_evolution_cycle(
        task_description=task_description,
        initial_metrics=initial_metrics
    )

# --- rl ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@rl_app.command("run", help="å¼·åŒ–å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè©¦è¡ŒéŒ¯èª¤ã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚")
def rl_run(
    episodes: int = typer.Option(100, help="å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°"),
    pattern_size: int = typer.Option(10, help="ç’°å¢ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚µã‚¤ã‚º")
):
    from tqdm import tqdm
    
    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    env = SimpleEnvironment(pattern_size=pattern_size, device=device)
    agent = ReinforcementLearnerAgent(input_size=pattern_size, output_size=pattern_size, device=device)
    
    progress_bar = tqdm(range(episodes))
    total_reward = 0.0

    for episode in progress_bar:
        state = env.reset()
        action = agent.get_action(state)
        _, reward, _ = env.step(action)
        agent.learn(reward)
        total_reward += reward
        avg_reward = total_reward / (episode + 1)
        progress_bar.set_postfix({"Avg Reward": f"{avg_reward:.3f}"})
    
    print(f"\nâœ… å­¦ç¿’å®Œäº†ã€‚æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {total_reward / episodes:.4f}")

# --- gradient-train ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£… ---
@app.command(
    "gradient-train",
    help="""
    å‹¾é…ãƒ™ãƒ¼ã‚¹ã§SNNãƒ¢ãƒ‡ãƒ«ã‚’æ‰‹å‹•å­¦ç¿’ã—ã¾ã™ (train.pyã‚’å‘¼ã³å‡ºã—ã¾ã™)ã€‚
    ã“ã®ã‚³ãƒãƒ³ãƒ‰ã®å¾Œã«ã€train.pyã«æ¸¡ã—ãŸã„å¼•æ•°ã‚’ãã®ã¾ã¾ç¶šã‘ã¦ãã ã•ã„ã€‚
    
    ä¾‹: `python snn-cli.py gradient-train --model_config configs/models/large.yaml --data_path data/sample_data.jsonl`
    """,
    context_settings={"allow_extra_args": True, "ignore_unknown_options": True}
)
def gradient_train(ctx: typer.Context):
    print("ğŸ”§ å‹¾é…ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™...")
    # ã“ã®ã‚³ãƒãƒ³ãƒ‰ä»¥é™ã®ã™ã¹ã¦ã®å¼•æ•°ã‚’å–å¾—
    train_args = ctx.args
    
    # train.py ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã« sys.argv ã‚’ä¸€æ™‚çš„ã«æ›¸ãæ›ãˆã‚‹
    original_argv = sys.argv
    # æœ€åˆã®å¼•æ•°ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆåã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€'train.py' ã‚’è¨­å®š
    sys.argv = ["train.py"] + train_args
    
    try:
        gradient_based_trainer.main()
    finally:
        # å®Ÿè¡ŒãŒçµ‚ã‚ã£ãŸã‚‰ sys.argv ã‚’å…ƒã«æˆ»ã™
        sys.argv = original_argv

if __name__ == "__main__":
    app()
