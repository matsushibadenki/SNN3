# /snn-cli.py
# Title: çµ±åˆCLIãƒ„ãƒ¼ãƒ«
# Description: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨æ©Ÿèƒ½ã‚’ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚

import argparse
import sys
from pathlib import Path
from snn_research.core.snn_core import SNNCore # å¤‰æ›´
from snn_research.training.trainers import BPTTTrainer
from snn_research.data.datasets import SpikingDataset
import typer
from omegaconf import OmegaConf
import torch
import os



# --- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ  ---
sys.path.append(str(Path(__file__).resolve().parent))

# --- å„æ©Ÿèƒ½ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from snn_research.agent import AutonomousAgent, SelfEvolvingAgent, DigitalLifeForm, ReinforcementLearnerAgent
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner
from snn_research.rl_env.simple_env import SimpleEnvironment
import train as gradient_based_trainer # train.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ



app = typer.Typer()

@app.command()
def train(
    config_path: str = typer.Option("configs/base_config.yaml", help="Path to the base config file."),
    model_config_path: str = typer.Option(..., help="Path to the model config file (e.g., configs/models/spiking_transformer.yaml)."),
    output_dir: str = typer.Option("models/", help="Directory to save the trained model.")
):
    """SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    print("--- Starting Training ---")
    
    # è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒãƒ¼ã‚¸
    cfg = OmegaConf.load(config_path)
    model_cfg = OmegaConf.load(model_config_path)
    cfg.merge_with(model_cfg)
    print("Configuration loaded:")
    print(OmegaConf.to_yaml(cfg))

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    # vocab_sizeã‚’ãƒ¢ãƒ‡ãƒ«è¨­å®šã‹ã‚‰å–å¾—
    vocab_size = cfg.model.params.get("vocab_size", 1000) if cfg.model.type == 'spiking_transformer' else 100
    dataset = SpikingDataset(num_samples=100, sequence_length=32, vocab_size=vocab_size)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.training.batch_size)
    print(f"Dataset prepared with {len(dataset)} samples.")

    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    model = SNNCore(cfg)
    print("Model initialized:")
    print(model)

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    trainer = BPTTTrainer(model, cfg)
    print("Trainer initialized.")

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("--- Training Loop ---")
    for epoch in range(cfg.training.epochs):
        total_loss = 0
        for i, (data, targets) in enumerate(dataloader):
            loss = trainer.train_step(data, targets)
            total_loss += loss
            if i % 10 == 0:
                print(f"Epoch [{epoch+1}/{cfg.training.epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss:.4f}")
        print(f"Epoch {epoch+1} Average Loss: {total_loss / len(dataloader):.4f}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    model_name = cfg.model.get("type", "snn_model")
    save_path = os.path.join(output_dir, f"{model_name}_final.pth")
    torch.save(model.state_dict(), save_path)
    print(f"--- Training Finished ---")
    print(f"Model saved to {save_path}")


# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
def handle_agent(args):
    """è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å—ã‘å–ã£ãŸå“è³ªåŸºæº–ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    agent = AutonomousAgent(
        accuracy_threshold=args.min_accuracy,
        energy_budget=args.max_spikes
    )
    selected_model_info = agent.handle_task(
        task_description=args.task,
        unlabeled_data_path=args.unlabeled_data_path,
        force_retrain=args.force_retrain
    )
    if selected_model_info and args.prompt:
        print("\n" + "="*20 + " ğŸ§  INFERENCE " + "="*20)
        print(f"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        agent.run_inference(selected_model_info, args.prompt)
    elif not selected_model_info:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)
        print("ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

def handle_planner(args):
    """éšå±¤çš„ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    planner = HierarchicalPlanner()
    final_result = planner.execute_task(
        task_request=args.request,
        context=args.context
    )
    if final_result:
        print("\n" + "="*20 + " âœ… FINAL RESULT " + "="*20)
        print(final_result)
    else:
        print("\n" + "="*20 + " âŒ TASK FAILED " + "="*20)

def handle_life_form(args):
    """ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    life_form = DigitalLifeForm(project_root=".")
    life_form.awareness_loop(cycles=args.cycles)

def handle_evolution(args):
    """è‡ªå·±é€²åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    agent = SelfEvolvingAgent(project_root=".", model_config_path=args.model_config)
    initial_metrics = {
        "accuracy": args.initial_accuracy,
        "avg_spikes_per_sample": args.initial_spikes
    }
    agent.run_evolution_cycle(
        task_description=args.task_description,
        initial_metrics=initial_metrics
    )

def handle_rl(args):
    """ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    # run_rl_agent.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«çµ±åˆ
    import torch
    from tqdm import tqdm

    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    env = SimpleEnvironment(pattern_size=args.pattern_size, device=device)
    agent = ReinforcementLearnerAgent(input_size=args.pattern_size, output_size=args.pattern_size, device=device)
    
    progress_bar = tqdm(range(args.episodes))
    total_reward = 0.0

    for episode in progress_bar:
        state = env.reset()
        action = agent.get_action(state)
        _, reward, _ = env.step(action)
        agent.learn(reward)
        total_reward += reward
        avg_reward = total_reward / (episode + 1)
        progress_bar.set_postfix({"Avg Reward": f"{avg_reward:.3f}"})
    
    print(f"\nâœ… å­¦ç¿’å®Œäº†ã€‚æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {total_reward / args.episodes:.4f}")

def handle_train(args):
    """å‹¾é…ãƒ™ãƒ¼ã‚¹å­¦ç¿’ã®æ©Ÿèƒ½ã‚’å‡¦ç†ã™ã‚‹"""
    # train.py ã® main() é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    # train.pyå†…ã®DIã‚³ãƒ³ãƒ†ãƒŠè¨­å®šãŒãã®ã¾ã¾åˆ©ç”¨ã•ã‚Œã‚‹
    print("ğŸ”§ å‹¾é…ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # train.pyã®mainé–¢æ•°ãŒå¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ãŸã‚ã€sys.argvã‚’ä¸€æ™‚çš„ã«æ›¸ãæ›ãˆã‚‹
    original_argv = sys.argv
    sys.argv = [original_argv[0]] + args.train_args
    gradient_based_trainer.main()
    sys.argv = original_argv # å…ƒã«æˆ»ã™

def train(
    config_path: str = typer.Option("configs/base_config.yaml", help="Path to the base config file."),
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    model_config_path: str = typer.Option(..., help="Path to the model config file (e.g., configs/models/spiking_transformer.yaml)."),
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    output_dir: str = typer.Option("models/", help="Directory to save the trained model.")
):
    # ... (configã®ãƒ­ãƒ¼ãƒ‰å‡¦ç†)
    cfg = OmegaConf.load(config_path)
    model_cfg = OmegaConf.load(model_config_path)
    cfg.merge_with(model_cfg) # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒãƒ¼ã‚¸

    # ... (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã€ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    # SNNCoreãŒè¨­å®šã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
    model = SNNCore(cfg) 
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    
    # ... (å­¦ç¿’ãƒ«ãƒ¼ãƒ—)

def main():
    parser = argparse.ArgumentParser(
        description="Project SNN: çµ±åˆCLIãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawTextHelpFormatter
    )
    subparsers = parser.add_subparsers(dest="command", required=True, help="å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ")

    # --- Agent Subcommand ---
    parser_agent = subparsers.add_parser("agent", help="è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ“ä½œã—ã¦å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
    parser_agent.add_argument("solve", help="æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã—ã¾ã™")
    parser_agent.add_argument("--task", type=str, required=True, help="ã‚¿ã‚¹ã‚¯ã®è‡ªç„¶è¨€èªèª¬æ˜ (ä¾‹: 'æ„Ÿæƒ…åˆ†æ')")
    parser_agent.add_argument("--prompt", type=str, help="æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser_agent.add_argument("--unlabeled_data_path", type=str, help="æ–°è¦å­¦ç¿’æ™‚ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹")
    parser_agent.add_argument("--force_retrain", action="store_true", help="ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ç°¿ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶çš„ã«å†å­¦ç¿’")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    parser_agent.add_argument("--min_accuracy", type=float, default=0.6, help="å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®æœ€ä½ç²¾åº¦è¦ä»¶ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.6)")
    parser_agent.add_argument("--max_spikes", type=float, default=10000.0, help="å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®å¹³å‡ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10000.0)")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    parser_agent.set_defaults(func=handle_agent)

    # --- Planner Subcommand ---
    parser_planner = subparsers.add_parser("planner", help="é«˜æ¬¡èªçŸ¥ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’æ“ä½œã—ã¦è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ")
    parser_planner.add_argument("execute", help="è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¦æ±‚ã‚’å®Ÿè¡Œã—ã¾ã™")
    parser_planner.add_argument("--request", type=str, required=True, help="ã‚¿ã‚¹ã‚¯è¦æ±‚ (ä¾‹: 'è¨˜äº‹ã‚’è¦ç´„ã—ã¦æ„Ÿæƒ…ã‚’åˆ†æ')")
    parser_planner.add_argument("--context", type=str, required=True, help="å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿")
    parser_planner.set_defaults(func=handle_planner)

    # --- Life Form Subcommand ---
    parser_life = subparsers.add_parser("life-form", help="ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®è‡ªå¾‹ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹")
    parser_life.add_argument("start", help="æ„è­˜ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™")
    parser_life.add_argument("--cycles", type=int, default=5, help="å®Ÿè¡Œã™ã‚‹æ„è­˜ã‚µã‚¤ã‚¯ãƒ«ã®å›æ•°")
    parser_life.set_defaults(func=handle_life_form)

    # --- Evolution Subcommand ---
    parser_evo = subparsers.add_parser("evolve", help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ")
    parser_evo.add_argument("run", help="è‡ªå·±é€²åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’1å›å®Ÿè¡Œã—ã¾ã™")
    parser_evo.add_argument("--task_description", type=str, required=True, help="è‡ªå·±è©•ä¾¡ã®èµ·ç‚¹ã¨ãªã‚‹ã‚¿ã‚¹ã‚¯èª¬æ˜")
    parser_evo.add_argument("--model_config", type=str, default="configs/models/small.yaml", help="é€²åŒ–å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser_evo.add_argument("--initial_accuracy", type=float, default=0.75, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸç²¾åº¦")
    parser_evo.add_argument("--initial_spikes", type=float, default=1500.0, help="è‡ªå·±è©•ä¾¡ã®ãŸã‚ã®åˆæœŸã‚¹ãƒ‘ã‚¤ã‚¯æ•°")
    parser_evo.set_defaults(func=handle_evolution)
    
    # --- Reinforcement Learning Subcommand ---
    parser_rl = subparsers.add_parser("rl", help="ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œ")
    parser_rl.add_argument("run", help="å¼·åŒ–å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™")
    parser_rl.add_argument("--episodes", type=int, default=100, help="å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°")
    parser_rl.add_argument("--pattern_size", type=int, default=10, help="ç’°å¢ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚µã‚¤ã‚º")
    parser_rl.set_defaults(func=handle_rl)

    # --- Train Subcommand ---
    # æ®‹ã‚Šã®å¼•æ•°ã‚’ã™ã¹ã¦train.pyã«æ¸¡ã™ãŸã‚ã®ãƒ‘ãƒ¼ã‚µãƒ¼
    parser_train = subparsers.add_parser("train", help="å‹¾é…ãƒ™ãƒ¼ã‚¹ã§SNNãƒ¢ãƒ‡ãƒ«ã‚’æ‰‹å‹•å­¦ç¿’ (train.pyã®å¼•æ•°ã‚’æŒ‡å®š)")
    parser_train.add_argument('train_args', nargs=argparse.REMAINDER, help="train.pyã«æ¸¡ã™å¼•æ•° (ä¾‹: --config ...)")
    parser_train.set_defaults(func=handle_train)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
