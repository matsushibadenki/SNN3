# snn_research/tools/web_crawler.py
# Title: Web Crawler Tool
# Description: æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰Webãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚

import requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin, urlparse
from typing import Set, List, Optional
import time
import os
import json

class WebCrawler:
    """
    Webã‚’å·¡å›ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã€‚
    """
    def __init__(self, output_dir: str = "workspace/web_data"):
        self.visited_urls: Set[str] = set()
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    def _is_valid_url(self, url: str, base_domain: str) -> bool:
        """å·¡å›å¯¾è±¡ã¨ã—ã¦é©åˆ‡ãªURLã‹åˆ¤æ–­ã™ã‚‹ã€‚"""
        parsed_url = urlparse(url)
        return (
            parsed_url.scheme in ["http", "https"] and
            parsed_url.netloc == base_domain and
            url not in self.visited_urls
        )

    def _extract_text_from_html(self, html_content: str) -> str:
        """BeautifulSoupã‚’ä½¿ã£ã¦HTMLã‹ã‚‰ä¸»è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        soup = BeautifulSoup(html_content, 'html.parser')
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®ä¸è¦ãªéƒ¨åˆ†ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
            element.decompose()
        
        text = soup.get_text(separator='\n', strip=True)
        return text

    def crawl(self, start_url: str, max_pages: int = 5) -> str:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã€åé›†ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚

        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸjsonlãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
        """
        urls_to_visit: List[str] = [start_url]
        base_domain = urlparse(start_url).netloc
        
        output_filename = f"crawled_data_{int(time.time())}.jsonl"
        output_filepath = os.path.join(self.output_dir, output_filename)

        page_count = 0
        with open(output_filepath, 'w', encoding='utf-8') as f:
            while urls_to_visit and page_count < max_pages:
                current_url = urls_to_visit.pop(0)
                if not self._is_valid_url(current_url, base_domain):
                    continue

                try:
                    print(f"ğŸ“„ ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {current_url}")
                    response = requests.get(current_url, timeout=10)
                    response.raise_for_status()
                    self.visited_urls.add(current_url)
                    page_count += 1

                    text_content = self._extract_text_from_html(response.text)
                    
                    if text_content:
                        # ãƒ‡ãƒ¼ã‚¿ã‚’jsonlå½¢å¼ã§ä¿å­˜
                        record = {"text": text_content, "source_url": current_url}
                        f.write(json.dumps(record) + "\n")

                    # ãƒšãƒ¼ã‚¸å†…ã®æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    soup = BeautifulSoup(response.text, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                        # mypyãŒlink['href']ã§ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™ãŸã‚ã€ã‚ˆã‚Šå®‰å…¨ãªæ–¹æ³•ã§å±æ€§ã‚’å–å¾—
                        if isinstance(link, Tag):
                            href = link.get('href')
                            if isinstance(href, str):
                                absolute_link = urljoin(current_url, href)
                                if self._is_valid_url(absolute_link, base_domain):
                                    urls_to_visit.append(absolute_link)
                        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                    
                    time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è»½æ¸›

                except requests.RequestException as e:
                    print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {current_url} ({e})")

        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ã€‚{page_count}ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’ '{output_filepath}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return output_filepath

