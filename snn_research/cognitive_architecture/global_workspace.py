# matsushibadenki/snn2/snn_research/cognitive_architecture/global_workspace.py
# Phase 3: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹

from snn_research.distillation.model_registry import ModelRegistry
from snn_research.deployment import SNNInferenceEngine
from snn_research.agent.memory import Memory
from .rag_snn import RAGSystem
from typing import Optional, Dict, Any
import torch

class GlobalWorkspace:
    """
    è¤‡æ•°ã®å°‚é–€å®¶SNNãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ã—ã€æ€è€ƒã®ä¸­æ ¸ã‚’æ‹…ã†ã€‚
    RAGã‚·ã‚¹ãƒ†ãƒ ã¨é€£æºã—ã¦çŸ¥è­˜ã‚’æ´»ç”¨ã™ã‚‹ã€‚
    """
    def __init__(self) -> None:
        self.registry = ModelRegistry()
        self.memory = Memory()
        self.rag_system = RAGSystem()
        self.active_specialists: Dict[str, SNNInferenceEngine] = {}

    def _load_specialist(self, task_description: str) -> Optional[SNNInferenceEngine]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ã—ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
        """
        if task_description in self.active_specialists:
            return self.active_specialists[task_description]

        # Registryã‹ã‚‰æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ (ç¾æ™‚ç‚¹ã§ã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ)
        candidate_models = self.registry.find_models_for_task(task_description)
        if not candidate_models:
            return None
        
        best_model_info = candidate_models[0] # ã“ã“ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹é¸æŠã‚‚å¯èƒ½
        model_path = best_model_info['model_path']
        
        print(f"ğŸ§  ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãŒå°‚é–€å®¶ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
        self.memory.add_entry("SPECIALIST_LOADED", {"task": task_description, "model_path": model_path})

        engine = SNNInferenceEngine(
            model_path=model_path,
            device="mps" if torch.backends.mps.is_available() else "cpu"
        )
        self.active_specialists[task_description] = engine
        return engine

    def process_sub_task(self, sub_task: str, context: str) -> Optional[str]:
        """
        å˜ä¸€ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        å°‚é–€å®¶ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€RAGã‚·ã‚¹ãƒ†ãƒ ã«å•ã„åˆã‚ã›ã‚‹ã€‚
        """
        specialist = self._load_specialist(sub_task)
        if not specialist:
            print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ '{sub_task}' ã‚’å®Ÿè¡Œã§ãã‚‹å°‚é–€å®¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã«å•ã„åˆã‚ã›ã¾ã™...")
            self.memory.add_entry("SPECIALIST_NOT_FOUND", {"task": sub_task})
            
            # RAGã§é–¢é€£æƒ…å ±ã‚’æ¤œç´¢
            rag_query = f"ã‚¿ã‚¹ã‚¯ã€Œ{sub_task}ã€ã®å®Ÿè¡Œæ–¹æ³•ã«ã¤ã„ã¦"
            rag_results = self.rag_system.search(rag_query)
            
            knowledge = "\n\n".join(rag_results)
            print(f"ğŸ” RAGã‹ã‚‰ã®çŸ¥è­˜:\n---\n{knowledge}\n---")
            self.memory.add_entry("RAG_SEARCH_PERFORMED", {"query": rag_query, "results": knowledge})
            
            # ç¾çŠ¶ã§ã¯å­¦ç¿’ã‚’ãƒˆãƒªã‚¬ãƒ¼ã›ãšã€çŸ¥è­˜ã‚’åŸºã«ã—ãŸå¿œç­”ã‚’è¿”ã™
            return f"å°‚é–€å®¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–¢é€£çŸ¥è­˜ã«ã‚ˆã‚‹ã¨ã€ã‚¿ã‚¹ã‚¯ '{sub_task}' ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‡¦ç†ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {knowledge}"


        print(f"ğŸ¤– å°‚é–€å®¶ '{sub_task}' ãŒå¿œç­”ã‚’ç”Ÿæˆä¸­...")
        self.memory.add_entry("SUB_TASK_STARTED", {"sub_task": sub_task, "context": context})
        
        full_response = ""
        for chunk in specialist.generate(context, max_len=150):
            full_response += chunk
        
        self.memory.add_entry("SUB_TASK_COMPLETED", {"sub_task": sub_task, "response": full_response})
        print(f"   - å¿œç­”: {full_response.strip()}")
        return full_response.strip()