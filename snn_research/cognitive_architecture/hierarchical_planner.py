# matsushibadenki/snn3/snn_research/cognitive_architecture/hierarchical_planner.py
# Phase 3: éšå±¤çš„æ€è€ƒãƒ—ãƒ©ãƒ³ãƒŠãƒ¼
#
# å¤‰æ›´ç‚¹:
# - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è¨ˆç”»ç«‹æ¡ˆã‚’æ’¤å»ƒã€‚
# - ModelRegistryã¨é€£æºã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ãªã‚¹ã‚­ãƒ«ï¼ˆå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ï¼‰ã«åŸºã¥ã„ã¦
#   å‹•çš„ã«å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã«å¤‰æ›´ã€‚
# - [æ”¹å–„] å­¦ç¿’æ¸ˆã¿ã®ã€Œãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNã€ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€è¨ˆç”»ç«‹æ¡ˆã‚’çŸ¥èƒ½åŒ–ã€‚

import torch
import os
from transformers import AutoTokenizer

from .global_workspace import GlobalWorkspace
from snn_research.agent.memory import Memory
from snn_research.distillation.model_registry import ModelRegistry
from .planner_snn import PlannerSNN
from typing import Optional, Dict, Any, List

class HierarchicalPlanner:
    """
    è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£ã—ã€GlobalWorkspaceã¨é€£æºã—ã¦å®Ÿè¡Œã‚’ç®¡ç†ã™ã‚‹ã€‚
    è‡ªå·±ã®èƒ½åŠ›ï¼ˆåˆ©ç”¨å¯èƒ½ãªå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ï¼‰ã«åŸºã¥ãã€å‹•çš„ã«è¨ˆç”»ã‚’ç«‹æ¡ˆã™ã‚‹ã€‚
    """
    def __init__(self, planner_model_path: str = "runs/planner_snn.pth"):
        self.workspace = GlobalWorkspace()
        self.memory = Memory()
        self.registry = ModelRegistry()
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2") # ã‚³ãƒ³ãƒ•ã‚£ã‚°ã‹ã‚‰å–å¾—ã™ã‚‹ã®ãŒæœ›ã¾ã—ã„
        self.available_skills = list(self.registry.registry.keys())
        self.skill_to_id = {skill: i for i, skill in enumerate(self.available_skills)}
        self.id_to_skill = {i: skill for skill, i in self.skill_to_id.items()}

        self.planner_snn = self._load_planner_model(planner_model_path)

    def _load_planner_model(self, model_path: str) -> Optional[PlannerSNN]:
        """å­¦ç¿’æ¸ˆã¿ã®ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚"""
        if not self.available_skills or not os.path.exists(model_path):
            print("âš ï¸ ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€åˆ©ç”¨å¯èƒ½ãªã‚¹ã‚­ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç¾åœ¨ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã¯ãƒ€ãƒŸãƒ¼ï¼ˆæœ¬æ¥ã¯DIã‚³ãƒ³ãƒ†ãƒŠçµŒç”±ã§å–å¾—ï¼‰
        model_config: Dict[str, int] = {'d_model': 128, 'd_state': 64, 'num_layers': 4, 'time_steps': 20, 'n_head': 2}
        
        planner_model = PlannerSNN(
            vocab_size=self.tokenizer.vocab_size,
            d_model=model_config['d_model'],
            d_state=model_config['d_state'],
            num_layers=model_config['num_layers'],
            time_steps=model_config['time_steps'],
            n_head=model_config['n_head'],
            num_skills=len(self.available_skills)
        ).to(self.device)
        
        planner_model.load_state_dict(torch.load(model_path, map_location=self.device))
        planner_model.eval()
        print("âœ… å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNã‚’æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        return planner_model

    @torch.no_grad()
    def _create_plan(self, task_request: str) -> List[str]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNã‚’ç”¨ã„ã¦ã€å®Ÿè¡Œè¨ˆç”»ã‚’å‹•çš„ã«æ¨è«–ã™ã‚‹ã€‚
        """
        print("ğŸ“ ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNãŒå®Ÿè¡Œè¨ˆç”»ã‚’æ¨è«–ä¸­...")
        if not self.planner_snn or not self.available_skills:
            return []

        input_ids = self.tokenizer.encode(
            task_request, return_tensors='pt',
            max_length=self.planner_snn.time_steps,
            padding='max_length', truncation=True
        ).to(self.device)

        # ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
        skill_logits, _, _ = self.planner_snn(input_ids)
        
        # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¹ã‚­ãƒ«ã‚’é †ç•ªã«é¸æŠ (è¤‡æ•°ã‚¹ã‚­ãƒ«ã‚’äºˆæ¸¬ã™ã‚‹å ´åˆ)
        # ã“ã“ã§ã¯ç°¡å˜ã®ãŸã‚ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„2ã¤ã‚’é¸æŠ
        predicted_skill_ids = torch.topk(skill_logits, k=min(2, len(self.available_skills)), dim=-1).indices.squeeze().tolist()
        
        plan = [self.id_to_skill[skill_id] for skill_id in predicted_skill_ids if skill_id in self.id_to_skill]
        
        self.memory.add_entry("PLAN_CREATED", {"request": task_request, "available_skills": self.available_skills, "plan": plan})
        return plan

    def execute_task(self, task_request: str, context: str) -> Optional[str]:
        """
        ã‚¿ã‚¹ã‚¯ã®è¨ˆç”»ç«‹æ¡ˆã‹ã‚‰å®Ÿè¡Œã¾ã§ã‚’çµ±æ‹¬ã™ã‚‹ã€‚
        """
        self.memory.add_entry("HIGH_LEVEL_TASK_RECEIVED", {"request": task_request, "context": context})
        
        plan = self._create_plan(task_request)
        if not plan:
            print(f"âŒ ã‚¿ã‚¹ã‚¯ '{task_request}' ã«å¯¾ã™ã‚‹å®Ÿè¡Œè¨ˆç”»ã‚’ç«‹æ¡ˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            self.memory.add_entry("PLANNING_FAILED", {"request": task_request})
            return None

        print(f"âœ… å®Ÿè¡Œè¨ˆç”»ãŒæ±ºå®š: {' -> '.join(plan)}")
        
        current_context = context
        for sub_task in plan:
            print(f"\n Fase de ejecuciÃ³n de la subtarea: '{sub_task}'...")
            
            # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã«ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã‚’ä¾é ¼
            result = self.workspace.process_sub_task(sub_task, current_context)
            
            if result is None:
                print(f"âŒ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ '{sub_task}' ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                self.memory.add_entry("SUB_TASK_FAILED", {"sub_task": sub_task})
                return None
            
            current_context = result # æ¬¡ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®å…¥åŠ›ã¨ã—ã¦çµæœã‚’æ¸¡ã™
        
        return current_context
