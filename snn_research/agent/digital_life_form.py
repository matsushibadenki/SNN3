# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: matsushibadenki/snn3/SNN3-190ede29139f560c909685675a68ccf65069201c/snn_research/agent/digital_life_form.py
#
# DigitalLifeForm ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
#
# æ¦‚è¦ï¼šå†…ç™ºçš„å‹•æ©Ÿä»˜ã‘ã¨ãƒ¡ã‚¿èªçŸ¥ã«åŸºã¥ãã€å„ç¨®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è‡ªå¾‹çš„ã«èµ·å‹•ã™ã‚‹ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã€‚
#
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º7ã€Œè‡ªå·±è¨€åŠã€ã‚’å®Ÿè£…ã€‚
# - LangChainé€£æºæ©Ÿèƒ½ã‚’åˆ©ç”¨ã—ã€è‡ªèº«ã®è¡Œå‹•ãƒ­ã‚°ã‚’åŸºã«ã€
#   ã€Œãªãœãã®è¡Œå‹•ã‚’å–ã£ãŸã®ã‹ã€ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹`explain_last_action`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º7ã€Œè¨˜å·å‰µç™ºã€ã‚’å®Ÿè£…ã€‚
# - SymbolGroundingãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆã—ã€æœªçŸ¥ã®çµŒé¨“ã‹ã‚‰æ–°ã—ã„æ¦‚å¿µã‚’è‡ªå¾‹çš„ã«ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
#
# æ”¹å–„ç‚¹ (v2):
# - ä¾å­˜æ€§æ³¨å…¥ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€__init__ãƒ¡ã‚½ãƒƒãƒ‰ã§å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
#
# æ”¹å–„ç‚¹ (v3):
# - self.state ã®å‹ãƒ’ãƒ³ãƒˆã‚’æ˜ç¤ºã—ã€mypyã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã€‚

import time
import logging
import torch
import random
import json
from typing import Dict, Any, Optional

from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.meta_cognitive_snn import MetaCognitiveSNN
from snn_research.agent.memory import Memory
from snn_research.cognitive_architecture.physics_evaluator import PhysicsEvaluator
from snn_research.cognitive_architecture.symbol_grounding import SymbolGrounding
from app.containers import AppContainer
from snn_research.agent.autonomous_agent import AutonomousAgent
from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.agent.self_evolving_agent import SelfEvolvingAgent
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DigitalLifeForm:
    """
    å†…ç™ºçš„å‹•æ©Ÿä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ¡ã‚¿èªçŸ¥SNNã‚’çµ±åˆã—ã€
    æ°¸ç¶šçš„ã§è‡ªå·±é§†å‹•ã™ã‚‹å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿç¾ã™ã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚
    """
    def __init__(
        self,
        autonomous_agent: AutonomousAgent,
        rl_agent: ReinforcementLearnerAgent,
        self_evolving_agent: SelfEvolvingAgent,
        motivation_system: IntrinsicMotivationSystem,
        meta_cognitive_snn: MetaCognitiveSNN,
        memory: Memory,
        physics_evaluator: PhysicsEvaluator,
        symbol_grounding: SymbolGrounding,
        app_container: AppContainer
    ):
        self.autonomous_agent = autonomous_agent
        self.rl_agent = rl_agent
        self.self_evolving_agent = self_evolving_agent
        self.motivation_system = motivation_system
        self.meta_cognitive_snn = meta_cognitive_snn
        self.memory = memory
        self.physics_evaluator = physics_evaluator
        self.symbol_grounding = symbol_grounding
        self.app_container = app_container
        
        self.running = False
        # mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: å‹ãƒ’ãƒ³ãƒˆã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        self.state: Dict[str, Any] = {"last_action": None, "last_result": None}


    def start(self):
        self.running = True
        logging.info("DigitalLifeForm activated. Starting autonomous loop.")
        self.life_cycle()

    def stop(self):
        self.running = False
        logging.info("DigitalLifeForm deactivating.")

    def life_cycle(self):
        while self.running:
            internal_state = self.motivation_system.get_internal_state()
            performance_eval = self.meta_cognitive_snn.evaluate_performance()
            dummy_mem_sequence = torch.randn(100)
            dummy_spikes = (torch.rand(100) > 0.8).float()
            physical_rewards = self.physics_evaluator.evaluate_physical_consistency(dummy_mem_sequence, dummy_spikes)
            
            action = self._decide_next_action(internal_state, performance_eval, physical_rewards)
            
            result, external_reward, expert_used = self._execute_action(action)

            self.symbol_grounding.process_observation(result, context=f"action '{action}'")
            
            reward_vector = {
                "external": external_reward,
                "physical": physical_rewards
            }
            decision_context = {"internal_state": internal_state, "performance_eval": performance_eval, "physical_rewards": physical_rewards}
            self.memory.record_experience(self.state, action, result, reward_vector, expert_used, decision_context)
            
            # ä»¥ä¸‹ã¯ãƒ€ãƒŸãƒ¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            dummy_prediction_error = result.get("prediction_error", 0.1) if isinstance(result, dict) else 0.1
            dummy_success_rate = result.get("success_rate", 0.9) if isinstance(result, dict) else 0.9
            dummy_task_similarity = 0.8
            dummy_loss = result.get("loss", 0.05) if isinstance(result, dict) else 0.05
            dummy_time = result.get("computation_time", 1.0) if isinstance(result, dict) else 1.0
            dummy_accuracy = result.get("accuracy", 0.95) if isinstance(result, dict) else 0.95

            self.motivation_system.update_metrics(dummy_prediction_error, dummy_success_rate, dummy_task_similarity, dummy_loss)
            self.meta_cognitive_snn.update_metadata(dummy_loss, dummy_time, dummy_accuracy)
            self.state = {"last_action": action, "last_result": result}
            
            logging.info(f"Action: {action}, Result: {result}, Reward: {external_reward}")
            logging.info(f"New Internal State: {self.motivation_system.get_internal_state()}")
            
            time.sleep(10)

    def _decide_next_action(self, internal_state: Dict[str, float], performance_eval: Dict[str, Any], physical_rewards: Dict[str, float]) -> str:
        action_scores: Dict[str, float] = {
            "acquire_new_knowledge": 0.0,
            "evolve_architecture": 0.0,
            "explore_new_task_with_rl": 0.0,
            "plan_and_execute": 0.0,
            "practice_skill_with_rl": 0.0,
        }

        if performance_eval.get("status") == "knowledge_gap":
            action_scores["acquire_new_knowledge"] += 10.0
            logging.info("Decision reason: Knowledge gap detected.")
        
        if performance_eval.get("status") == "capability_gap":
            action_scores["evolve_architecture"] += 5.0
            logging.info("Decision reason: Capability gap detected.")
        if physical_rewards.get("sparsity_reward", 1.0) < 0.5:
            action_scores["evolve_architecture"] += 8.0
            logging.info("Decision reason: Low energy efficiency (sparsity).")

        action_scores["explore_new_task_with_rl"] += internal_state.get("curiosity", 0.5) * 5.0
        action_scores["plan_and_execute"] += internal_state.get("curiosity", 0.5) * 3.0
        
        if internal_state.get("boredom", 0.0) > 0.7:
            action_scores["explore_new_task_with_rl"] += internal_state.get("boredom", 0.0) * 10.0
            logging.info("Decision reason: High boredom.")

        action_scores["practice_skill_with_rl"] += internal_state.get("confidence", 0.5) * 2.0
        action_scores["explore_new_task_with_rl"] += internal_state.get("confidence", 0.5) * 1.0

        action_scores["practice_skill_with_rl"] += 1.0

        actions = list(action_scores.keys())
        scores = [max(0, s) for s in action_scores.values()]
        total_score = sum(scores)

        if total_score == 0:
            chosen_action = "practice_skill_with_rl"
        else:
            probabilities = [s / total_score for s in scores]
            chosen_action = random.choices(actions, weights=probabilities, k=1)[0]
        
        logging.info(f"Action scores: {action_scores}")
        if total_score > 0:
            logging.info(f"Probabilities: { {a: f'{p:.2%}' for a, p in zip(actions, probabilities)} }")
        logging.info(f"Chosen action: {chosen_action}")

        return chosen_action

    def _execute_action(self, action: str) -> tuple[Dict[str, Any], float, List[str]]:
        try:
            if action == "acquire_new_knowledge":
                result_str = self.autonomous_agent.learn_from_web("latest SNN research trends")
                return {"status": "success", "info": result_str, "accuracy": 0.96}, 0.8, ["web_crawler"]
            elif action == "evolve_architecture":
                result_str = self.self_evolving_agent.evolve()
                return {"status": "success", "info": result_str, "accuracy": 0.97}, 0.9, ["self_evolver"]
            elif action == "explore_new_task_with_rl":
                return {"status": "success", "info": "Exploration finished.", "accuracy": 0.92}, 0.7, ["rl_agent_explorer"]
            elif action == "practice_skill_with_rl":
                return {"status": "success", "info": "Practice finished.", "accuracy": 0.98}, 0.5, ["rl_agent_practicer"]
            elif action == "plan_and_execute":
                result_str = self.autonomous_agent.execute("Summarize the latest trends in SNN and analyze the sentiment.")
                return {"status": "success", "info": result_str, "accuracy": 0.95}, 0.8, ["planner", "summarizer_snn", "sentiment_snn"]
            else:
                return {"status": "failed", "info": "Unknown action"}, 0.0, []
        except Exception as e:
            logging.error(f"Error executing action '{action}': {e}")
            return {"status": "error", "info": str(e)}, -1.0, []

    def awareness_loop(self, cycles: int):
        print(f"ğŸ§¬ Digital Life Form awareness loop starting for {cycles} cycles.")
        self.running = True
        for i in range(cycles):
            if not self.running:
                break
            print(f"\n----- Cycle {i+1}/{cycles} -----")
            self.life_cycle_step() # 1ã‚µã‚¤ã‚¯ãƒ«åˆ†ã®å‡¦ç†ã‚’å‘¼ã³å‡ºã—
            time.sleep(2)
        self.stop()
        print("ğŸ§¬ Awareness loop finished.")
    
    def life_cycle_step(self):
        """life_cycleã®1å›åˆ†ã®å‡¦ç†"""
        internal_state = self.motivation_system.get_internal_state()
        performance_eval = self.meta_cognitive_snn.evaluate_performance()
        dummy_mem_sequence = torch.randn(100)
        dummy_spikes = (torch.rand(100) > 0.8).float()
        physical_rewards = self.physics_evaluator.evaluate_physical_consistency(dummy_mem_sequence, dummy_spikes)
        action = self._decide_next_action(internal_state, performance_eval, physical_rewards)
        result, reward, expert_used = self._execute_action(action)
        
        reward_vector = {"external": reward, "physical": physical_rewards}
        decision_context = {"internal_state": internal_state, "performance_eval": performance_eval, "physical_rewards": physical_rewards}
        self.memory.record_experience(self.state, action, result, reward_vector, expert_used, decision_context)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        if isinstance(result, dict):
            self.motivation_system.update_metrics(result.get("prediction_error", 0.1), result.get("success_rate", 0.9), 0.8, result.get("loss", 0.05))
            self.meta_cognitive_snn.update_metadata(result.get("loss", 0.05), result.get("computation_time", 1.0), result.get("accuracy", 0.95))

        self.state = {"last_action": action, "last_result": result}
        logging.info(f"Action: {action}, Result: {result}, Reward: {reward}")

    def explain_last_action(self) -> Optional[str]:
        """
        ç›´è¿‘ã®è¡Œå‹•ãƒ­ã‚°ã‚’åŸºã«ã€è‡ªèº«ã®è¡Œå‹•ç†ç”±ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹ã€‚
        """
        try:
            with open(self.memory.memory_path, "rb") as f:
                try:
                    f.seek(-2, 2)
                    while f.read(1) != b'\n':
                        f.seek(-2, 1)
                except OSError:
                    f.seek(0)
                last_line = f.readline().decode()
            
            last_experience = json.loads(last_line)
        except (IOError, json.JSONDecodeError, IndexError):
            return "è¡Œå‹•å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

        prompt = f"""
        ã‚ãªãŸã¯ã€è‡ªèº«ã®è¡Œå‹•ã‚’åˆ†æã—ã€ãã®ç†ç”±ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹AIã§ã™ã€‚
        ä»¥ä¸‹ã®å†…éƒ¨ãƒ­ã‚°ã¯ã€ã‚ãªãŸè‡ªèº«ã®ç›´è¿‘ã®è¡Œå‹•è¨˜éŒ²ã§ã™ã€‚ã“ã®è¨˜éŒ²ã‚’åŸºã«ã€ãªãœãã®è¡Œå‹•ã‚’å–ã£ãŸã®ã‹ã‚’ä¸€äººç§°ï¼ˆã€Œç§ã€ï¼‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

        ### è¡Œå‹•ãƒ­ã‚°
        - **å®Ÿè¡Œã—ãŸè¡Œå‹•:** {last_experience.get('action')}
        - **æ„æ€æ±ºå®šã®æ ¹æ‹ :**
          - **å†…ç™ºçš„å‹•æ©Ÿï¼ˆå†…éƒ¨çŠ¶æ…‹ï¼‰:** {last_experience.get('decision_context', {}).get('internal_state')}
          - **è‡ªå·±ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡:** {last_experience.get('decision_context', {}).get('performance_eval')}
          - **ç‰©ç†åŠ¹ç‡è©•ä¾¡:** {last_experience.get('decision_context', {}).get('physical_rewards')}

        ### æŒ‡ç¤º
        ä¸Šè¨˜ã®æ ¹æ‹ ã‚’çµ±åˆã—ã€ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å¹³æ˜“ãªè¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        """
        print("\n--- è‡ªå·±è¨€åŠãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---")
        print(prompt)
        print("--------------------------\n")

        try:
            snn_llm = self.app_container.langchain_adapter()
            explanation = snn_llm._call(prompt)
            return explanation
        except Exception as e:
            logging.error(f"LLMã«ã‚ˆã‚‹è‡ªå·±è¨€åŠã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return "ã‚¨ãƒ©ãƒ¼: è‡ªå·±è¨€åŠã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
