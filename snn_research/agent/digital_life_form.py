# matsushibadenki/snn3/snn_research/agent/digital_life_form.py
#
# DigitalLifeForm ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
#
# æ¦‚è¦ï¼šå†…ç™ºçš„å‹•æ©Ÿä»˜ã‘ã¨ãƒ¡ã‚¿èªçŸ¥ã«åŸºã¥ãã€å„ç¨®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è‡ªå¾‹çš„ã«èµ·å‹•ã™ã‚‹ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã€‚
# mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: RLAgentã‚’ReinforcementLearnerAgentã«ä¿®æ­£ã€‚
# mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: snn-cli.pyã‹ã‚‰ã®å‘¼ã³å‡ºã—ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€awareness_loopãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã—ã€__init__ã‚’ä¿®æ­£ã€‚
# æ”¹å–„ç‚¹: ä¾å­˜é–¢ä¿‚ã‚’å…·è±¡ã‚¯ãƒ©ã‚¹ã§è§£æ±ºã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
#
# æ”¹å–„ç‚¹:
# - ROADMAP.mdã®ãƒ•ã‚§ãƒ¼ã‚º5ã«åŸºã¥ãã€PhysicsEvaluatorã‚’å°å…¥ã€‚
# - æ„æ€æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ã«ç‰©ç†æ³•å‰‡ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã€å‡¦ç†ã®æ»‘ã‚‰ã‹ã•ï¼‰ã®è©•ä¾¡ã‚’çµ„ã¿è¾¼ã¿ã€
#   ã‚ˆã‚Šé«˜åº¦ãªè‡ªå¾‹çš„åˆ¤æ–­ã‚’å¯èƒ½ã«ã—ãŸã€‚
#
# æ”¹å–„ç‚¹:
# - ROADMAP.mdã®ãƒ•ã‚§ãƒ¼ã‚º6ã«åŸºã¥ãã€æ„æ€æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¢ºç‡çš„é¸æŠãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ã€‚
# - è¤‡æ•°ã®å†…éƒ¨çŠ¶æ…‹ã‹ã‚‰å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã—ã€é‡ã¿ä»˜ã‘ã•ã‚ŒãŸç¢ºç‡ã§æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ã€‚
# - è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«å¤šç›®çš„å ±é…¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜éŒ²ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚

import time
import logging
import torch
import random
from typing import Dict, Any
from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.meta_cognitive_snn import MetaCognitiveSNN
from snn_research.agent.memory import Memory
from snn_research.cognitive_architecture.physics_evaluator import PhysicsEvaluator
# å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.agent.autonomous_agent import AutonomousAgent
from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.agent.self_evolving_agent import SelfEvolvingAgent
from snn_research.cognitive_architecture.planner_snn import PlannerSNN
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner
from snn_research.distillation.model_registry import SimpleModelRegistry
from snn_research.tools.web_crawler import WebCrawler
from snn_research.cognitive_architecture.rag_snn import RAGSystem


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DigitalLifeForm:
    """
    å†…ç™ºçš„å‹•æ©Ÿä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ¡ã‚¿èªçŸ¥SNNã‚’çµ±åˆã—ã€
    æ°¸ç¶šçš„ã§è‡ªå·±é§†å‹•ã™ã‚‹å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿç¾ã™ã‚‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚
    """
    def __init__(self):
        self.motivation_system = IntrinsicMotivationSystem()
        self.meta_cognitive_snn = MetaCognitiveSNN()
        self.memory = Memory()
        self.physics_evaluator = PhysicsEvaluator()
        
        # å…·è±¡ã‚¯ãƒ©ã‚¹ã§ä¾å­˜é–¢ä¿‚ã‚’è§£æ±º
        model_registry = SimpleModelRegistry()
        rag_system = RAGSystem()
        web_crawler = WebCrawler()
        
        # ä¾å­˜é–¢ä¿‚ã‚’æ³¨å…¥ã—ã¦ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆ
        # ToDo: å­¦ç¿’æ¸ˆã¿ã®PlannerSNNãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ä»•çµ„ã¿ã‚’è¿½åŠ ã™ã‚‹
        planner = HierarchicalPlanner(
            model_registry=model_registry,
            rag_system=rag_system,
            planner_model=None # ç¾æ™‚ç‚¹ã§ã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å‹•ä½œ
        )
        
        # å„ç¨®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        self.autonomous_agent = AutonomousAgent(name="AutonomousAgent", planner=planner, model_registry=model_registry, memory=self.memory, web_crawler=web_crawler)
        self.rl_agent = ReinforcementLearnerAgent(input_size=10, output_size=4, device="cpu") # Dummy sizes for now
        self.self_evolving_agent = SelfEvolvingAgent(
            name="SelfEvolvingAgent",
            planner=planner,
            model_registry=model_registry,
            memory=self.memory,
            web_crawler=web_crawler
        )
        
        self.running = False
        self.state = {"last_action": None} # ã‚·ã‚¹ãƒ†ãƒ ã®ç¾åœ¨ã®çŠ¶æ…‹

    def start(self):
        """ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®æ´»å‹•ã‚’é–‹å§‹ã™ã‚‹ã€‚"""
        self.running = True
        logging.info("DigitalLifeForm activated. Starting autonomous loop.")
        self.life_cycle()

    def stop(self):
        """ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®æ´»å‹•ã‚’åœæ­¢ã™ã‚‹ã€‚"""
        self.running = False
        logging.info("DigitalLifeForm deactivating.")

    def life_cycle(self):
        """
        ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œãƒ«ãƒ¼ãƒ—ã€‚
        å†…éƒ¨çŠ¶æ…‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã«åŸºã¥ãã€è‡ªå¾‹çš„ã«è¡Œå‹•ã‚’æ±ºå®šã—ç¶šã‘ã‚‹ã€‚
        """
        while self.running:
            # 1. å†…éƒ¨çŠ¶æ…‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’å–å¾—
            internal_state = self.motivation_system.get_internal_state()
            performance_eval = self.meta_cognitive_snn.evaluate_performance()
            # ç‰©ç†æ³•å‰‡ã®ä¸€è²«æ€§ã‚’è©•ä¾¡
            dummy_mem_sequence = torch.randn(100) # ãƒ€ãƒŸãƒ¼ã®è†œé›»ä½ç³»åˆ—
            dummy_spikes = (torch.rand(100) > 0.8).float() # ãƒ€ãƒŸãƒ¼ã®ã‚¹ãƒ‘ã‚¤ã‚¯
            physical_rewards = self.physics_evaluator.evaluate_physical_consistency(dummy_mem_sequence, dummy_spikes)
            
            # 2. çŠ¶æ…‹ã«åŸºã¥ãæ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
            action = self._decide_next_action(internal_state, performance_eval, physical_rewards)
            
            # 3. æ±ºå®šã—ãŸè¡Œå‹•ã‚’å®Ÿè¡Œ
            result, external_reward, expert_used = self._execute_action(action)

            # 4. çµŒé¨“ã‚’è¨˜éŒ² (å¤šç›®çš„å ±é…¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜éŒ²)
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            reward_vector = {
                "external": external_reward,
                "physical": physical_rewards
            }
            decision_context = {"internal_state": internal_state, "performance_eval": performance_eval, "physical_rewards": physical_rewards}
            self.memory.record_experience(self.state, action, result, reward_vector, expert_used, decision_context)
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            
            # 5. ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
            # ToDo: å®Ÿè¡Œçµæœã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
            dummy_prediction_error = result.get("prediction_error", 0.1)
            dummy_success_rate = result.get("success_rate", 0.9)
            dummy_task_similarity = 0.8 # å®Ÿéš›ã«ã¯ã‚¿ã‚¹ã‚¯é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            dummy_loss = result.get("loss", 0.05)
            dummy_time = result.get("computation_time", 1.0)
            dummy_accuracy = result.get("accuracy", 0.95)

            self.motivation_system.update_metrics(dummy_prediction_error, dummy_success_rate, dummy_task_similarity, dummy_loss)
            self.meta_cognitive_snn.update_metadata(dummy_loss, dummy_time, dummy_accuracy)
            self.state = {"last_action": action, "last_result": result}
            
            logging.info(f"Action: {action}, Result: {result}, Reward: {external_reward}")
            logging.info(f"New Internal State: {self.motivation_system.get_internal_state()}")
            
            time.sleep(10) # å®Ÿè¡Œé–“éš”

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def _decide_next_action(self, internal_state: Dict[str, float], performance_eval: Dict[str, Any], physical_rewards: Dict[str, float]) -> str:
        """
        çŠ¶æ…‹é·ç§»ãƒ­ã‚¸ãƒƒã‚¯ã€‚å†…éƒ¨çŠ¶æ…‹ã«åŸºã¥ãã€å„è¡Œå‹•ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€ç¢ºç‡çš„ã«æ¬¡ã®è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã€‚
        """
        action_scores = {
            "acquire_new_knowledge": 0.0,
            "evolve_architecture": 0.0,
            "explore_new_task_with_rl": 0.0,
            "plan_and_execute": 0.0,
            "practice_skill_with_rl": 0.0,
        }

        # --- ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ ---
        # çŸ¥è­˜ä¸è¶³ã®å ´åˆã€çŸ¥è­˜ç²å¾—ã®å„ªå…ˆåº¦ã‚’å¤§å¹…ã«ä¸Šã’ã‚‹
        if performance_eval["status"] == "knowledge_gap":
            action_scores["acquire_new_knowledge"] += 10.0
            logging.info("Decision reason: Knowledge gap detected.")
        
        # èƒ½åŠ›ä¸è¶³ã€ã¾ãŸã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ãŒæ‚ªã„å ´åˆã€è‡ªå·±é€²åŒ–ã®å„ªå…ˆåº¦ã‚’ä¸Šã’ã‚‹
        if performance_eval["status"] == "capability_gap":
            action_scores["evolve_architecture"] += 5.0
            logging.info("Decision reason: Capability gap detected.")
        if physical_rewards.get("sparsity_reward", 1.0) < 0.5:
            action_scores["evolve_architecture"] += 8.0
            logging.info("Decision reason: Low energy efficiency (sparsity).")

        # å¥½å¥‡å¿ƒã¯æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®æ¢æ±‚ã‚„è¤‡é›‘ãªè¨ˆç”»ã‚’ä¿ƒé€²
        action_scores["explore_new_task_with_rl"] += internal_state["curiosity"] * 5.0
        action_scores["plan_and_execute"] += internal_state["curiosity"] * 3.0
        
        # é€€å±ˆã—ã¦ã„ã‚‹å ´åˆã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®æ¢æ±‚ã‚’å¼·ãæ¨å¥¨
        if internal_state["boredom"] > 0.7:
            action_scores["explore_new_task_with_rl"] += internal_state["boredom"] * 10.0
            logging.info("Decision reason: High boredom.")

        # è‡ªä¿¡ãŒã‚ã‚‹å ´åˆã€æ—¢å­˜ã‚¹ã‚­ãƒ«ã®ç·´ç¿’ï¼ˆæ´»ç”¨ï¼‰ã‚ˆã‚Šã¯æ–°ã—ã„æŒ‘æˆ¦ã‚’å„ªå…ˆ
        action_scores["practice_skill_with_rl"] += internal_state["confidence"] * 2.0
        action_scores["explore_new_task_with_rl"] += internal_state["confidence"] * 1.0

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡Œå‹•ã¨ã—ã¦ã€å¸¸ã«ç·´ç¿’ã«ã¯å°ã•ãªã‚¹ã‚³ã‚¢ã‚’ä¸ãˆã‚‹
        action_scores["practice_skill_with_rl"] += 1.0

        # --- ç¢ºç‡çš„é¸æŠ ---
        actions = list(action_scores.keys())
        scores = [max(0, s) for s in action_scores.values()] # ã‚¹ã‚³ã‚¢ã¯éè² 
        total_score = sum(scores)

        if total_score == 0:
            chosen_action = "practice_skill_with_rl" # ã‚¹ã‚³ã‚¢ãŒå…¨ã¦0ãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            probabilities = [s / total_score for s in scores]
            chosen_action = random.choices(actions, weights=probabilities, k=1)[0]
        
        logging.info(f"Action scores: {action_scores}")
        logging.info(f"Probabilities: { {a: f'{p:.2%}' for a, p in zip(actions, probabilities) if total_score > 0} }")
        logging.info(f"Chosen action: {chosen_action}")

        return chosen_action
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def _execute_action(self, action):
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾å¿œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        try:
            if action == "acquire_new_knowledge":
                result_str = self.autonomous_agent.learn_from_web("latest SNN research trends")
                return {"status": "success", "info": result_str, "accuracy": 0.96}, 0.8, ["web_crawler"]
            elif action == "evolve_architecture":
                # ToDo: evolveãƒ¡ã‚½ãƒƒãƒ‰ã®å…·ä½“çš„ãªå®Ÿè£…ã¨é€£æº
                result_str = self.self_evolving_agent.evolve()
                return {"status": "success", "info": result_str, "accuracy": 0.97}, 0.9, ["self_evolver"]
            elif action == "explore_new_task_with_rl":
                # ToDo: RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ã¨é€£æº
                return {"status": "success", "info": "Exploration finished.", "accuracy": 0.92}, 0.7, ["rl_agent_explorer"]
            elif action == "practice_skill_with_rl":
                # ToDo: RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ã¨é€£æº
                return {"status": "success", "info": "Practice finished.", "accuracy": 0.98}, 0.5, ["rl_agent_practicer"]
            elif action == "plan_and_execute":
                result_str = self.autonomous_agent.execute("Summarize the latest trends in SNN and analyze the sentiment.")
                return {"status": "success", "info": result_str, "accuracy": 0.95}, 0.8, ["planner", "summarizer_snn", "sentiment_snn"]
            else:
                return {"status": "failed", "info": "Unknown action"}, 0.0, []
        except Exception as e:
            logging.error(f"Error executing action '{action}': {e}")
            return {"status": "error", "info": str(e)}, -1.0, []

    def awareness_loop(self, cycles: int):
        """
        snn-cliã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ãŸã‚ã®ç°¡æ˜“çš„ãªå®Ÿè¡Œãƒ«ãƒ¼ãƒ—ã€‚
        """
        print(f"ğŸ§¬ Digital Life Form awareness loop starting for {cycles} cycles.")
        self.running = True
        for i in range(cycles):
            if not self.running:
                break
            print(f"\n----- Cycle {i+1}/{cycles} -----")
            # life_cycleã¯å†…éƒ¨ã§ãƒ«ãƒ¼ãƒ—ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯1ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®å‡¦ç†ã‚’ç›´æ¥å‘¼ã³å‡ºã™
            internal_state = self.motivation_system.get_internal_state()
            performance_eval = self.meta_cognitive_snn.evaluate_performance()
            dummy_mem_sequence = torch.randn(100)
            dummy_spikes = (torch.rand(100) > 0.8).float()
            physical_rewards = self.physics_evaluator.evaluate_physical_consistency(dummy_mem_sequence, dummy_spikes)
            action = self._decide_next_action(internal_state, performance_eval, physical_rewards)
            result, reward, expert_used = self._execute_action(action)
            
            reward_vector = {"external": reward, "physical": physical_rewards}
            decision_context = {"internal_state": internal_state, "performance_eval": performance_eval, "physical_rewards": physical_rewards}
            self.memory.record_experience(self.state, action, result, reward_vector, expert_used, decision_context)
            
            # çŠ¶æ…‹æ›´æ–°ã®ãƒ€ãƒŸãƒ¼å‡¦ç†
            self.motivation_system.update_metrics(0.1, 0.9, 0.8, 0.05)
            self.meta_cognitive_snn.update_metadata(0.05, 1.0, 0.95)
            self.state = {"last_action": action, "last_result": result}
            
            logging.info(f"Action: {action}, Result: {result}, Reward: {reward}")
            time.sleep(2) # ã‚µã‚¤ã‚¯ãƒ«é–“ã®å¾…æ©Ÿ
        self.stop()
        print("ğŸ§¬ Awareness loop finished.")
