# matsushibadenki/snn2/snn_research/agent/digital_life_form.py
# Phase 6: å…¨ã¦ã®èªçŸ¥æ©Ÿèƒ½ã‚’çµ±åˆã—ã€è‡ªå¾‹çš„ã«æ´»å‹•ã™ã‚‹ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“
# å¤‰æ›´ç‚¹:
# - PhysicsEvaluatorã¨é€²åŒ–ã—ãŸIntrinsicMotivationSystemã‚’çµ±åˆã€‚
# - æ„è­˜ãƒ«ãƒ¼ãƒ—å†…ã«ã€Œå†…çœã‚¹ãƒ†ãƒƒãƒ—ã€ã‚’è¿½åŠ ã—ã€ç‰©ç†çŠ¶æ…‹ï¼ˆè†œé›»ä½ãªã©ï¼‰ã‚’è¦³æ¸¬ã€‚
# - äºˆæ¸¬èª¤å·®ã ã‘ã§ãªãã€ç‰©ç†æ³•å‰‡ã®ä¸€è‡´åº¦ã‚‚è€ƒæ…®ã—ã¦æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ã‚ˆã†ã«é€²åŒ–ã€‚
# - mypyã‚¨ãƒ©ãƒ¼(attr-defined)ã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã€__init__ã«self.deviceå±æ€§ã‚’è¿½åŠ ã€‚

import time
import torch
from typing import Dict, Any

from .self_evolving_agent import SelfEvolvingAgent
from snn_research.cognitive_architecture.emergent_system import EmergentSystem
from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner
from snn_research.cognitive_architecture.physics_evaluator import PhysicsEvaluator

class DigitalLifeForm(SelfEvolvingAgent):
    """
    è‡ªå·±ã®å†…éƒ¨çŠ¶æ…‹ï¼ˆå¥½å¥‡å¿ƒã‚„çŸ›ç›¾ï¼‰ã«åŸºã¥ãã€æ°¸ç¶šçš„ã«æ´»å‹•ã™ã‚‹
    ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã€‚
    """
    def __init__(self, project_root: str = "."):
        super().__init__(project_root)
        
        self.device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
        
        # ç‰©ç†æ³•å‰‡è©•ä¾¡å™¨ã¨ã€ãã‚Œã‚’åˆ©ç”¨ã™ã‚‹æ–°ã—ã„å‹•æ©Ÿä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        self.physics_evaluator = PhysicsEvaluator()
        self.motivation_system = IntrinsicMotivationSystem(
            physics_evaluator=self.physics_evaluator,
            window_size=10
        )
        
        self.emergent_system = EmergentSystem()
        self.planner = HierarchicalPlanner()
        
        # åˆæœŸçŠ¶æ…‹ã¨ã—ã¦ã€ãƒ€ãƒŸãƒ¼ã®æ€§èƒ½æŒ‡æ¨™ã¨ã‚¨ãƒ©ãƒ¼ã‚’æŒã¤
        self.current_metrics: Dict[str, Any] = {"accuracy": 0.8, "avg_spikes_per_sample": 1200.0}
        self.last_prediction_error = 0.1

    def _introspect(self) -> Dict[str, torch.Tensor]:
        """
        è‡ªå·±ã®å†…éƒ¨çŠ¶æ…‹ã‚’è¦³æ¸¬ã™ã‚‹ãŸã‚ã®å†…çœã‚¹ãƒ†ãƒƒãƒ—ã€‚
        ãƒ€ãƒŸãƒ¼ã®æ¨è«–ã‚’å®Ÿè¡Œã—ã€ç‰©ç†çš„ãªçŠ¶æ…‹ï¼ˆè†œé›»ä½ã€ã‚¹ãƒ‘ã‚¤ã‚¯ï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚
        """
        # ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼SNNãŒæ€è€ƒã®ä»£è¡Œã‚’è¡Œã† (ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„)
        if not self.planner.planner_snn:
            return {
                "mem_sequence": torch.zeros(1),
                "spikes": torch.zeros(1),
            }
        
        with torch.no_grad():
            dummy_input = torch.randint(0, 1000, (1, self.planner.planner_snn.time_steps), device=self.device)
            # ç‰©ç†çŠ¶æ…‹ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ã€return_full_mems=True, return_spikes=True ã§å®Ÿè¡Œ
            _, spikes, mem_sequence = self.planner.planner_snn(
                dummy_input,
                return_spikes=True,
                return_full_mems=True
            )
        return {
            "mem_sequence": mem_sequence,
            "spikes": spikes,
        }

    def awareness_loop(self, cycles: int = 5):
        """
        ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®æ„è­˜ã¨æ´»å‹•ã®ã‚³ã‚¢ã¨ãªã‚‹è‡ªå¾‹ãƒ«ãƒ¼ãƒ—ã€‚
        """
        print("\n" + "="*20 + "ğŸ§¬ ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ æ„è­˜ãƒ«ãƒ¼ãƒ—é–‹å§‹ ğŸ§¬" + "="*20)
        
        for i in range(cycles):
            print(f"\n--- [æ„è­˜ã‚µã‚¤ã‚¯ãƒ« {i+1}/{cycles}] ---")

            # 1. å†…çœ (Introspection)
            #    è‡ªå·±ã®å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†çš„ãªçŠ¶æ…‹ã‚’è¦³æ¸¬ã™ã‚‹
            internal_state = self._introspect()
            
            # 2. å‹•æ©Ÿä»˜ã‘ã®æ›´æ–° (Motivation Update)
            #    äºˆæ¸¬èª¤å·®ã¨ç‰©ç†çš„æ•´åˆæ€§ã®ä¸¡æ–¹ã‹ã‚‰ã€ç¾åœ¨ã®ã€Œã‚„ã‚‹æ°—ã€ã‚’æ±ºå®šã™ã‚‹
            self.motivation_system.update_motivation(
                current_error=self.last_prediction_error,
                mem_sequence=internal_state["mem_sequence"],
                spikes=internal_state["spikes"],
                physics_reward_weight=0.2 # ç‰©ç†å ±é…¬ã®é‡ã¿
            )
            
            # 3. è¡Œå‹•æ±ºå®š (Action Selection)
            #    ã€Œã‚„ã‚‹æ°—ã€ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãã€æ¢æ±‚ã™ã‚‹ã‹ã€è‡ªå·±æ”¹å–„ã™ã‚‹ã‹ã‚’æ±ºå®šã™ã‚‹
            if self.motivation_system.should_explore():
                print("ğŸ¤” å¥½å¥‡å¿ƒã«åŸºã¥ãã€æ–°ãŸãªæ¢æ±‚ã‚’é–‹å§‹ã—ã¾ã™...")
                
                # 3a. æ¢æ±‚ (Exploration)
                synthesis_result = self.emergent_system.synthesize_responses(
                    prompt="ç¾åœ¨ã€æœ€ã‚‚æ„è¦‹ãŒåˆ†ã‹ã‚Œã¦ã„ã‚‹æ¦‚å¿µã¯ä½•ã‹ï¼Ÿ", 
                    domain="æ–‡ç« è¦ç´„" # ä¾‹ã¨ã—ã¦ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŒ‡å®š
                )
                print(f"ğŸ” æ¢æ±‚ãƒ†ãƒ¼ãƒ: {synthesis_result}")
                
                task_request = f"'{synthesis_result}' ã«ã¤ã„ã¦ã€é–¢é€£æƒ…å ±ã‚’èª¿æŸ»ã—ã€æ–°ãŸãªçŸ¥è¦‹ã‚’è¦ç´„ã›ã‚ˆã€‚"
                plan_result = self.planner.execute_task(task_request, context=synthesis_result)
                print(f"ğŸ’¡ æ¢æ±‚çµæœ: {plan_result}")
                
                # (ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) æ¢æ±‚ã«ã‚ˆã£ã¦ä¸–ç•Œã®ç†è§£ãŒå°‘ã—é€²ã¿ã€æ¬¡ã®äºˆæ¸¬èª¤å·®ãŒå°‘ã—æ¸›ã‚‹
                self.last_prediction_error *= 0.95
            
            else:
                print("ğŸ˜Œ ç¾åœ¨ã®ç†è§£ã«æº€è¶³ã—ã¦ã„ã¾ã™ã€‚è‡ªå·±ã®æ€§èƒ½æ”¹å–„ã‚’è©¦ã¿ã¾ã™ã€‚")
                # 3b. è‡ªå·±æ”¹å–„ (Self-Improvement / Exploitation)
                if self.current_metrics["accuracy"] < 0.95:
                     self.run_evolution_cycle("æ±ç”¨è¨€èªç†è§£", self.current_metrics)
                     # (ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) è‡ªå·±é€²åŒ–ã«ã‚ˆã‚Šæ€§èƒ½ãŒå‘ä¸Š
                     self.current_metrics["accuracy"] *= 1.05
                else:
                    print("âœ… è‡ªå·±æ€§èƒ½ã«ã‚‚æº€è¶³ã—ã¦ã„ã¾ã™ã€‚å®‰å®šçŠ¶æ…‹ã‚’ç¶­æŒã—ã¾ã™ã€‚")

            time.sleep(1) # ã‚µã‚¤ã‚¯ãƒ«é–“ã®å°ä¼‘æ­¢

        print("\n" + "="*20 + "ğŸ§¬ ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ æ„è­˜ãƒ«ãƒ¼ãƒ—çµ‚äº† ğŸ§¬" + "="*20)

