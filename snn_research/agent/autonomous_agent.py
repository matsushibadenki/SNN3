# matsushibadenki/snn3/snn_research/agent/autonomous_agent.py
# Phase 0 ã®è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
#
# å¤‰æ›´ç‚¹:
# - é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ (Memory)ã‚’çµ±åˆã—ã€å…¨ã¦ã®æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# - é¸æŠã—ãŸå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ `run_inference` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
# - SNNInferenceEngineã‚’å‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã€‚

import torch
from .memory import Memory
from snn_research.distillation.model_registry import ModelRegistry
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from snn_research.deployment import SNNInferenceEngine
from typing import Optional, Dict, Any

class AutonomousAgent:
    """
    ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦æœ€é©ãªå°‚é–€å®¶SNNã‚’é¸æŠã€ã¾ãŸã¯ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    å…¨ã¦ã®è¡Œå‹•ã¯Memoryã«è¨˜éŒ²ã•ã‚Œã‚‹ã€‚
    """
    def __init__(self, accuracy_threshold: float = 0.6, energy_budget: float = 1000.0):
        self.registry = ModelRegistry()
        self.memory = Memory()
        self.distillation_manager = KnowledgeDistillationManager(
            base_config_path="configs/base_config.yaml",
            model_config_path="configs/models/small.yaml" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        )
        self.accuracy_threshold = accuracy_threshold
        self.energy_budget = energy_budget # å¹³å‡ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã®ä¸Šé™
        self.active_model: Optional[SNNInferenceEngine] = None

    def _select_best_model(self, task_description: str) -> Optional[Dict[str, Any]]:
        """
        ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã¨æ€§èƒ½ã«åŸºã¥ã„ã¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã€‚
        """
        self.memory.add_entry("MODEL_SELECTION_STARTED", {"task": task_description})
        candidate_models = self.registry.find_models_for_task(task_description)
        if not candidate_models:
            self.memory.add_entry("MODEL_SELECTION_ENDED", {"result": "no_candidates_found"})
            return None

        best_model = None
        highest_score = -1

        print(f"ğŸ§  {len(candidate_models)}å€‹ã®å€™è£œãƒ¢ãƒ‡ãƒ«ã®ä¸­ã‹ã‚‰æœ€é©ãªå°‚é–€å®¶ã‚’é¸å®šä¸­...")
        for model_info in candidate_models:
            metrics = model_info['metrics']
            accuracy = metrics.get('accuracy', 0)
            spikes = metrics.get('avg_spikes_per_sample', float('inf'))

            if accuracy >= self.accuracy_threshold and spikes <= self.energy_budget:
                score = (accuracy * 100) - (spikes / self.energy_budget) 
                print(f"  - å€™è£œ: {model_info['model_path']} (Accuracy: {accuracy:.4f}, Spikes: {spikes:,.0f}, Score: {score:.2f})")
                if score > highest_score:
                    highest_score = score
                    best_model = model_info
        
        self.memory.add_entry("MODEL_SELECTION_ENDED", {
            "task": task_description,
            "best_model_found": best_model['model_path'] if best_model else "None",
            "score": highest_score
        })
        return best_model

    def handle_task(self, task_description: str, unlabeled_data_path: Optional[str], force_retrain: bool) -> Optional[Dict[str, Any]]:
        """
        ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã™ã‚‹ãƒ¡ã‚¤ãƒ³ã®ãƒ¡ã‚½ãƒƒãƒ‰ã€‚
        """
        self.memory.add_entry("TASK_RECEIVED", {
            "task": task_description,
            "unlabeled_data_path": unlabeled_data_path,
            "force_retrain": force_retrain
        })
        print(f"å—ã‘å–ã£ãŸã‚¿ã‚¹ã‚¯: '{task_description}'")

        if not force_retrain:
            selected_model = self._select_best_model(task_description)
            if selected_model:
                print(f"âœ… æœ€é©ãªå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’é¸å®šã—ã¾ã—ãŸ: {selected_model['model_path']}")
                return selected_model

        print(f"æœ€é©ãªå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å†å­¦ç¿’ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸã€‚")
        
        if not unlabeled_data_path:
            error_details = "æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã¯ --unlabeled_data_path ãŒå¿…è¦ã§ã™ã€‚"
            print(f"âŒ {error_details}")
            self.memory.add_entry("ERROR_ENCOUNTERED", {"reason": error_details})
            return None

        # æ–°ã—ã„å°‚é–€å®¶ã‚’è‚²æˆ
        self.memory.add_entry("TRAINING_INITIATED", {"task": task_description})
        self.distillation_manager.run_on_demand_pipeline(
            task_description=task_description,
            unlabeled_data_path=unlabeled_data_path,
            teacher_model_name="gpt2" # å°†æ¥çš„ã«ã¯å‹•çš„ã«é¸æŠ
        )
        self.memory.add_entry("TRAINING_COMPLETED", {"task": task_description})
        
        # å­¦ç¿’å¾Œã€å†åº¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦è¿”ã™
        return self._select_best_model(task_description)
        
    def run_inference(self, model_info: Dict[str, Any], prompt: str):
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã™ã‚‹ã€‚
        """
        model_path = model_info['model_path']
        self.memory.add_entry("INFERENCE_STARTED", {"model_path": model_path, "prompt": prompt})
        
        if not self.active_model or self.active_model.model_path != model_path:
            print(f"ğŸ§  æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path}")
            self.active_model = SNNInferenceEngine(
                model_path=model_path,
                device="mps" if torch.backends.mps.is_available() else "cpu"
            )
        
        print("ğŸ¤– å¿œç­”:")
        full_response = ""
        for chunk in self.active_model.generate(prompt, max_len=100):
            print(chunk, end="", flush=True)
            full_response += chunk
        print()
        
        self.memory.add_entry("INFERENCE_COMPLETED", {"model_path": model_path, "response": full_response})
        return full_response