# matsushibadenki/snn3/snn_research/distillation/knowledge_distillation_manager.py
# Title: çŸ¥è­˜è’¸ç•™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# Description: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¸ã®çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ç®¡ç†ãƒ»å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
# mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: å­˜åœ¨ã—ãªã„`get`ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã‚’ä¿®æ­£ã€‚
# mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: register_modelã®å¼•æ•°ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã«å¤‰æ›´ã€‚
# mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: __init__ã‚’ä¿®æ­£ã—ã€å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã™ã¹ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
# æ”¹å–„ç‚¹: run_on_demand_pipelineãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ–°è¦å®Ÿè£…ã€‚

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedModel, PreTrainedTokenizer, AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, Optional, List
import asyncio
import os
import json
from tqdm import tqdm

from snn_research.training.trainers import DistillationTrainer
from snn_research.distillation.model_registry import ModelRegistry
from snn_research.benchmark.metrics import calculate_perplexity, calculate_energy_consumption

class KnowledgeDistillationManager:
    """
    çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ±æ‹¬ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(
        self,
        student_model: torch.nn.Module,
        trainer: DistillationTrainer,
        teacher_model_name: str,
        tokenizer_name: str,
        model_registry: ModelRegistry,
        device: str
    ):
        self.student_model = student_model.to(device)
        self.distillation_trainer = trainer
        self.teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model_registry = model_registry
        self.device = device

    def prepare_dataset(self, texts: List[str], max_length: int, batch_size: int) -> DataLoader:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŸ¥è­˜è’¸ç•™ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æº–å‚™ã™ã‚‹ã€‚
        """
        class _DistillationTextDataset(Dataset):
            def __init__(self, tokenizer, texts, max_length, teacher_model, device):
                self.tokenizer = tokenizer
                self.texts = texts
                self.max_length = max_length
                self.teacher_model = teacher_model
                self.device = device

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                text = self.texts[idx]
                tokenized = self.tokenizer(
                    text,
                    padding='max_length',
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                input_ids = tokenized['input_ids'].squeeze(0)
                
                # Note: æœ¬æ¥ã¯äº‹å‰è¨ˆç®—ãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯å‹•çš„ã«ãƒ­ã‚¸ãƒƒãƒˆã‚’ç”Ÿæˆ
                with torch.no_grad():
                    teacher_logits = self.teacher_model(input_ids.unsqueeze(0).to(self.device)).logits.squeeze(0).cpu()
                
                return {
                    'input_ids': input_ids,
                    'attention_mask': tokenized['attention_mask'].squeeze(),
                    'teacher_logits': teacher_logits
                }

        dataset = _DistillationTextDataset(self.tokenizer, texts, max_length, self.teacher_model, self.device)
        
        def collate_fn(batch):
            input_ids = torch.stack([item['input_ids'] for item in batch])
            targets = torch.roll(input_ids, shifts=-1, dims=1)
            targets[:, -1] = self.tokenizer.pad_token_id
            teacher_logits = torch.stack([item['teacher_logits'] for item in batch])
            return input_ids, targets, teacher_logits

        return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)


    async def run_distillation(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_id: str,
        task_description: str,
        student_config: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        çŸ¥è­˜è’¸ç•™ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        """
        print(f"--- Starting Knowledge Distillation for model: {model_id} ---")

        # 1. çŸ¥è­˜è’¸ç•™ã®å®Ÿè¡Œ
        print("Step 1: Running distillation training...")
        # trainer.trainã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€trainerã®train_epochã¨evaluateã‚’ç›´æ¥å‘¼ã³å‡ºã™
        for epoch in range(epochs):
            self.distillation_trainer.train_epoch(train_loader, epoch)
            final_metrics = self.distillation_trainer.evaluate(val_loader, epoch)
        print("Distillation training finished.")

        # 2. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ (æœ€çµ‚)
        print("Step 2: Evaluating the distilled model...")
        evaluation_results = await self.evaluate_model(val_loader)
        final_metrics.update(evaluation_results)
        print(f"Evaluation finished. Metrics: {final_metrics}")

        # 3. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        save_dir = os.path.join("runs", "specialists", task_description.replace(" ", "_"))
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, "best_model.pth")
        print(f"Step 3: Saving the model to {save_path}...")
        # DDPã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
        model_state_dict = self.student_model.module.state_dict() if hasattr(self.student_model, 'module') else self.student_model.state_dict()
        torch.save(model_state_dict, save_path)
        print("Model saved.")

        # 4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ç™»éŒ²
        print("Step 4: Registering the model...")
        await self.model_registry.register_model(
            model_id=model_id,
            task_description=task_description,
            metrics=final_metrics,
            model_path=save_path,
            config=student_config
        )
        print(f"Model '{model_id}' successfully registered.")

        print("--- Knowledge Distillation Finished ---")
        return {"model_id": model_id, "metrics": final_metrics, "path": save_path, "config": student_config}

    async def run_on_demand_pipeline(self, task_description: str, unlabeled_data_path: str, force_retrain: bool):
        """Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ç­‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚"""
        print(f"ğŸš€ Starting on-demand pipeline for task: {task_description}")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        texts = []
        with open(unlabeled_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    texts.append(json.loads(line)['text'])
                except (json.JSONDecodeError, KeyError):
                    # JSONLå½¢å¼ã§ãªã„å ´åˆã‚‚è€ƒæ…®
                    if line.strip():
                        texts.append(line.strip())
        
        if not texts:
            print("âŒ No text found in the provided data file. Aborting.")
            return

        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
        # ToDo: DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰è¨­å®šã‚’å–å¾—ã™ã‚‹
        max_len = 128
        batch_size = 4
        train_loader = self.prepare_dataset(texts, max_length=max_len, batch_size=batch_size)
        
        # 3. è’¸ç•™å®Ÿè¡Œ
        await self.run_distillation(
            train_loader=train_loader,
            val_loader=train_loader, # ç°¡æ˜“çš„ã«åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            epochs=5, # ãƒ‡ãƒ¢ç”¨ã«ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¨­å®š
            model_id=task_description,
            task_description=f"Expert for {task_description}",
            student_config={} # ToDo: ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å–å¾—
        )

    async def evaluate_model(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è’¸ç•™æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
        """
        self.student_model.eval()
        total_spikes = 0
        total_samples = 0

        progress_bar = tqdm(dataloader, desc="Evaluating Distilled Model")
        for batch in progress_bar:
            # Dataloaderã®å‡ºåŠ›å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´
            inputs, _, _ = batch
            inputs = inputs.to(self.device)

            with torch.no_grad():
                # BreakthroughTrainerã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‚è€ƒã«ä¿®æ­£
                model_to_eval = self.student_model.module if hasattr(self.student_model, 'module') else self.student_model
                outputs = model_to_eval(inputs, return_spikes=True)
                if isinstance(outputs, tuple) and len(outputs) > 1:
                    _, spikes, _ = outputs
                else:
                    spikes = torch.tensor(0.0)

            total_spikes += spikes.sum().item()
            total_samples += inputs.size(0)

        avg_spikes_per_sample = total_spikes / total_samples if total_samples > 0 else 0

        # ToDo: ã‚ˆã‚Šæ­£ç¢ºãªãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã®è¨ˆç®—
        perplexity = calculate_perplexity(self.student_model, dataloader, self.device)
        energy = calculate_energy_consumption(avg_spikes_per_sample)

        return {
            "perplexity": perplexity,
            "avg_spikes_per_sample": avg_spikes_per_sample,
            "estimated_energy_consumption": energy
        }
