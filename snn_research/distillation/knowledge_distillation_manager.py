# matsushibadenki/snn2/snn_research/distillation/knowledge_distillation_manager.py
# è‡ªå¾‹çš„ãªçŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
#
# å¤‰æ›´ç‚¹:
# - ModelRegistryã¨é€£æºã—ã€é‡è¤‡å­¦ç¿’ã®å›é¿ã¨å­¦ç¿’çµæœã®è‡ªå‹•ç™»éŒ²ã‚’è¡Œã†ã‚ˆã†ã«ã—ãŸã€‚
# - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®å‡ºåŠ›ã‚’æ­£è¦è¡¨ç¾ã§ãƒ‘ãƒ¼ã‚¹ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
# - [æ”¹å–„] è©•ä¾¡æ™‚ã«ã€å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã™ã‚ˆã†ã«ä¿®æ­£ã€‚
# - [æ”¹å–„] run_on_demand_pipelineã«force_retrainå¼•æ•°ã‚’è¿½åŠ ã—ã€ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ç°¿ã®ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã‚ˆã†ã«ä¿®æ­£ã€‚

import os
import re
import subprocess
import yaml
from typing import Dict, Any
from .model_registry import ModelRegistry

class KnowledgeDistillationManager:
    """
    Phase 0 ã®ä¸­æ ¸ã¨ãªã‚‹ã€ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã®çŸ¥è­˜è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç®¡ç†ã™ã‚‹ã€‚
    """
    def __init__(self, base_config_path: str, model_config_path: str):
        self.base_config_path = base_config_path
        self.model_config_path = model_config_path
        
        with open(base_config_path, 'r') as f:
            self.base_config: Dict[str, Any] = yaml.safe_load(f)
        with open(model_config_path, 'r') as f:
            self.model_config: Dict[str, Any] = yaml.safe_load(f)
            
        self.registry = ModelRegistry()

    def _run_command(self, command: list[str]) -> str:
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€æ¨™æº–å‡ºåŠ›ã‚’è¿”ã™ã€‚"""
        # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
        print("\n" + "="*20 + f" ğŸš€ EXECUTING: {' '.join(command)} " + "="*20)
        try:
            result = subprocess.run(
                command, check=True, capture_output=True, encoding='utf-8', text=True
            )
            print(result.stdout)
            if result.stderr:
                print("--- STDERR ---")
                print(result.stderr)
            return result.stdout
        except subprocess.CalledProcessError as e:
            print(f"âŒ ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("--- STDOUT ---")
            print(e.stdout)
            print("--- STDERR ---")
            print(e.stderr)
            raise
        finally:
            print("="*60 + "\n")

    def _parse_benchmark_results(self, output: str) -> Dict[str, float]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å‡ºåŠ›ã‹ã‚‰SNNã®æ€§èƒ½æŒ‡æ¨™ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
        metrics = {}
        try:
            # SNNã®çµæœè¡Œã‚’è¦‹ã¤ã‘ã‚‹ (ã‚ˆã‚ŠæŸ”è»Ÿãªæ­£è¦è¡¨ç¾)
            snn_results_str = re.search(r"SNN\s+([\d\.]+)\s+([\d\.]+)\s+([\d\.,NA/]+)", output, re.IGNORECASE)
            if snn_results_str:
                accuracy = float(snn_results_str.group(1))
                avg_latency_ms = float(snn_results_str.group(2))
                spikes_str = snn_results_str.group(3).replace(',', '')
                avg_spikes = float(spikes_str) if 'n/a' not in spikes_str.lower() else 0.0
                
                metrics = {
                    "accuracy": accuracy,
                    "avg_latency_ms": avg_latency_ms,
                    "avg_spikes_per_sample": avg_spikes
                }
        except (AttributeError, IndexError, ValueError) as e:
            print(f"âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\nOutput:\n{output}")
        return metrics

    def _evaluate_and_register_model(self, task_description: str, task_run_dir: str):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€çµæœã‚’ç™»éŒ²ç°¿ã«ç™»éŒ²ã™ã‚‹ã€‚"""
        # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
        print("ğŸ“Š å­¦ç¿’æ¸ˆã¿SNNãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")
        
        best_model_src = os.path.join(task_run_dir, 'best_model.pth')
        
        if not os.path.exists(best_model_src):
             print(f"âš ï¸ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {best_model_src}")
             return

        # å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        benchmark_output = self._run_command([
            "python", "scripts/run_benchmark.py",
            "--model_path", best_model_src
        ])
        metrics = self._parse_benchmark_results(benchmark_output)
        
        if not metrics:
            print("âš ï¸ æ€§èƒ½æŒ‡æ¨™ã‚’å–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        self.registry.register_model(
            task_description=task_description,
            model_path=best_model_src,
            metrics=metrics,
            config=self.model_config['model']
        )
        print("ğŸ† æ€§èƒ½è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    def run_on_demand_pipeline(self, task_description: str, unlabeled_data_path: str, teacher_model_name: str, force_retrain: bool = False):
        """æœªçŸ¥ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã€è‡ªå¾‹çš„ã«å°‚é–€å®¶SNNã‚’ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚"""
        
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # Step 0: Check model registry unless retraining is forced
        if not force_retrain:
            existing_models = self.registry.find_models_for_task(task_description)
            if existing_models:
                print(f"âœ… ã‚¿ã‚¹ã‚¯ '{task_description}' ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

        task_id = task_description.replace(' ', '_').lower()
        distillation_data_dir = f"precomputed_data/{task_id}"
        task_run_dir = f"runs/specialists/{task_id}" # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒ­ã‚°/ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        # --- ã‚¹ãƒ†ãƒƒãƒ—1: çŸ¥è­˜è’¸ç•™ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---
        self._run_command([
            "python", "scripts/prepare_distillation_data.py",
            "--input_file", unlabeled_data_path,
            "--output_dir", distillation_data_dir,
            "--teacher_model", teacher_model_name
        ])

        # --- ã‚¹ãƒ†ãƒƒãƒ—2: å°‚é–€å®¶SNNã®å­¦ç¿’ ---
        self._run_command([
            "python", "train.py",
            "--config", self.base_config_path,
            "--model_config", self.model_config_path,
            "--data_path", distillation_data_dir,
            "--override_config", f"training.type=distillation",
            "--override_config", f"training.log_dir={task_run_dir}"
        ])
        
        print("âœ… å°‚é–€å®¶SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # --- ã‚¹ãƒ†ãƒƒãƒ—3: è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«ç™»éŒ² ---
        self._evaluate_and_register_model(task_description, task_run_dir)

        print("ğŸ‰ å…¨ã¦ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")