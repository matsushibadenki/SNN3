# matsushibadenki/snn3/snn_research/distillation/knowledge_distillation_manager.py
# ã‚¿ã‚¤ãƒˆãƒ«: çŸ¥è­˜è’¸ç•™ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# æ©Ÿèƒ½èª¬æ˜Ž: å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€åž‹ãƒã‚§ãƒƒã‚¯æ™‚ã®ã¿DistillationTrainerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedModel, PreTrainedTokenizer, AutoModelForCausalLM, AutoTokenizer
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
from typing import Dict, Any, Optional, List, TYPE_CHECKING
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
import asyncio
import os
import json
from tqdm import tqdm

# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
# from snn_research.training.trainers import DistillationTrainer
from snn_research.distillation.model_registry import ModelRegistry
from snn_research.benchmark.metrics import calculate_perplexity, calculate_energy_consumption

# --- å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ¶ˆã®ãŸã‚ã®ä¿®æ­£ ---
# åž‹ãƒã‚§ãƒƒã‚¯æ™‚ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã€å®Ÿè¡Œæ™‚ã®å¾ªç’°å‚ç…§ã‚’å›žé¿ã™ã‚‹
if TYPE_CHECKING:
    from snn_research.training.trainers import DistillationTrainer
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

class KnowledgeDistillationManager:
    """
    çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ±æ‹¬ã™ã‚‹ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(
        self,
        student_model: torch.nn.Module,
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        trainer: "DistillationTrainer",
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        teacher_model_name: str,
        tokenizer_name: str,
        model_registry: ModelRegistry,
        device: str
    ):
        self.student_model = student_model.to(device)
        self.distillation_trainer = trainer
        self.teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model_registry = model_registry
        self.device = device

    def prepare_dataset(self, texts: List[str], max_length: int, batch_size: int) -> DataLoader:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŸ¥è­˜è’¸ç•™ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æº–å‚™ã™ã‚‹ã€‚
        """
        class _DistillationTextDataset(Dataset):
            def __init__(self, tokenizer, texts, max_length, teacher_model, device):
                self.tokenizer = tokenizer
                self.texts = texts
                self.max_length = max_length
                self.teacher_model = teacher_model
                self.device = device

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                text = self.texts[idx]
                tokenized = self.tokenizer(
                    text,
                    padding='max_length',
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                input_ids = tokenized['input_ids'].squeeze(0)
                
                # Note: æœ¬æ¥ã¯äº‹å‰è¨ˆç®—ãŒæœ›ã¾ã—ã„ãŒã€ã“ã“ã§ã¯å‹•çš„ã«ãƒ­ã‚¸ãƒƒãƒˆã‚’ç”Ÿæˆ
                with torch.no_grad():
                    teacher_logits = self.teacher_model(input_ids.unsqueeze(0).to(self.device)).logits.squeeze(0).cpu()
                
                return {
                    'input_ids': input_ids,
                    'attention_mask': tokenized['attention_mask'].squeeze(),
                    'teacher_logits': teacher_logits
                }

        dataset = _DistillationTextDataset(self.tokenizer, texts, max_length, self.teacher_model, self.device)
        
        def collate_fn(batch):
            input_ids = torch.stack([item['input_ids'] for item in batch])
            targets = torch.roll(input_ids, shifts=-1, dims=1)
            targets[:, -1] = self.tokenizer.pad_token_id
            teacher_logits = torch.stack([item['teacher_logits'] for item in batch])
            return input_ids, targets, teacher_logits

        return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)


    async def run_distillation(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_id: str,
        task_description: str,
        student_config: Dict[str, Any],
    ) -> Dict[str, Any]:
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŠã‚ˆã³ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚­ãƒ¼ã¨ã—ã¦å®‰å…¨ãªIDã‚’ç”Ÿæˆ
        safe_model_id = model_id.lower().replace(" ", "_")
        print(f"--- Starting Knowledge Distillation for model: {safe_model_id} ---")


        """
        çŸ¥è­˜è’¸ç•™ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        """
        print(f"--- Starting Knowledge Distillation for model: {model_id} ---")
        final_metrics: Dict[str, float] = {}

        # 1. çŸ¥è­˜è’¸ç•™ã®å®Ÿè¡Œ
        print("Step 1: Running distillation training...")
        for epoch in range(epochs):
            self.distillation_trainer.train_epoch(train_loader, epoch)
            final_metrics = self.distillation_trainer.evaluate(val_loader, epoch)
        print("Distillation training finished.")

        # 2. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ (æœ€çµ‚)
        print("Step 2: Evaluating the distilled model...")
        evaluation_results = await self.evaluate_model(val_loader)
        final_metrics.update(evaluation_results)
        print(f"Evaluation finished. Metrics: {final_metrics}")

        # 3. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã—ã¦å®‰å…¨ãªIDã‚’ç”Ÿæˆ (å°æ–‡å­—åŒ–ã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«)
        # ã“ã‚Œã«ã‚ˆã‚Šã€å¸¸ã«ä¸€è²«ã—ãŸãƒ‘ã‚¹ãŒç”Ÿæˆãƒ»ç™»éŒ²ã•ã‚Œã‚‹
        save_dir = os.path.join("runs", "specialists", safe_model_id)
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, "best_model.pth")
        print(f"Step 3: Saving the model to {save_path}...")
        
        # DDPã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€trainerã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        model_to_save = self.distillation_trainer.model
        model_state_dict = model_to_save.module.state_dict() if isinstance(model_to_save, nn.parallel.DistributedDataParallel) else model_to_save.state_dict()
        torch.save(model_state_dict, save_path)
        print("Model saved.")

        # 4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ç™»éŒ²
        print("Step 4: Registering the model...")
        # ç™»éŒ²æ™‚ã‚‚ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸIDã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
        await self.model_registry.register_model(
            model_id=safe_model_id,
            task_description=task_description,
            metrics=final_metrics,
            model_path=save_path,
            config=student_config
        )
        print(f"Model '{safe_model_id}' successfully registered.")

        print("--- Knowledge Distillation Finished ---")
        return {"model_id": safe_model_id, "metrics": final_metrics, "path": save_path, "config": student_config}

    async def run_on_demand_pipeline(self, task_description: str, unlabeled_data_path: str, force_retrain: bool, student_config: Optional[Dict[str, Any]] = None):
        """Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ç­‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚ªãƒ³ãƒ‡ãƒžãƒ³ãƒ‰å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚"""
        print(f"ðŸš€ Starting on-demand pipeline for task: {task_description}")

        if not student_config:
            raise ValueError("student_config must be provided for the on-demand learning pipeline.")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        texts = []
        with open(unlabeled_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    texts.append(json.loads(line)['text'])
                except (json.JSONDecodeError, KeyError):
                    if line.strip():
                        texts.append(line.strip())
        
        if not texts:
            print("âŒ No text found in the provided data file. Aborting.")
            return

        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
        # student_configãŒNoneã§ãªã„ã“ã¨ã‚’ä¿è¨¼
        max_len = student_config.get("time_steps", 128) if student_config else 128
        batch_size = 4 # ãƒ‡ãƒ¢ç”¨ã«å›ºå®š
        train_loader = self.prepare_dataset(texts, max_length=max_len, batch_size=batch_size)
        
        # 3. è’¸ç•™å®Ÿè¡Œ (ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—åŠ )
        await self.run_distillation(
            train_loader=train_loader,
            val_loader=train_loader,
            epochs=15,
            model_id=task_description,
            task_description=f"Expert for {task_description}",
            student_config=student_config
        )

    async def evaluate_model(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è’¸ç•™æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
        """
        model_to_eval = self.distillation_trainer.model
        model_to_eval.eval()
        total_spikes = 0
        total_samples = 0

        progress_bar = tqdm(dataloader, desc="Evaluating Distilled Model")
        for batch in progress_bar:
            inputs, _, _ = batch
            inputs = inputs.to(self.device)

            with torch.no_grad():
                outputs = model_to_eval(inputs, return_spikes=True)
                if isinstance(outputs, tuple) and len(outputs) > 1:
                    _, spikes, _ = outputs
                else:
                    # mypyã‚¨ãƒ©ãƒ¼ã‚’å›žé¿ã™ã‚‹ãŸã‚ã€torch.zerosã‚’ä½¿ç”¨
                    spikes = torch.zeros((), device=inputs.device)

            total_spikes += spikes.sum().item()
            total_samples += inputs.size(0)

        avg_spikes_per_sample = total_spikes / total_samples if total_samples > 0 else 0

        perplexity = calculate_perplexity(model_to_eval, dataloader, self.device)
        energy = calculate_energy_consumption(avg_spikes_per_sample)

        return {
            "perplexity": perplexity,
            "avg_spikes_per_sample": avg_spikes_per_sample,
            "estimated_energy_consumption": energy
        }