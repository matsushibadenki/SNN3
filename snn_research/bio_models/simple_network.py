# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/bio_models/simple_network.py
# ã‚¿ã‚¤ãƒˆãƒ«: BioSNN (ç”Ÿç‰©å­¦çš„SNN)
# (çœç•¥...)
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2ã€Œéšå±¤çš„å› æœå­¦ç¿’ã€ã«åŸºã¥ãã€è¤‡æ•°å±¤ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«æ‹¡å¼µã€‚
# - update_weightsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¸€èˆ¬åŒ–ã—ã€æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®ä¿¡ç”¨å‰²ã‚Šå½“ã¦ã‚’å¯èƒ½ã«ã—ãŸã€‚
# æ”¹å–„ç‚¹ (v2): ã€Œé©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã€ã‚’å®Ÿè£…ã€‚è²¢çŒ®åº¦ã®ä½ã„ã‚·ãƒŠãƒ—ã‚¹ã®å­¦ç¿’ã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚
# æ”¹å–„ç‚¹ (v3): ã€Œéšå±¤çš„å› æœã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦ã€ã‚’å®Ÿè£…ã€‚
#              å¾Œæ®µã®å±¤ã‹ã‚‰ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’å‰æ®µã®å±¤ã«é€†ä¼æ’­ã•ã›ã‚‹ã€‚

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple, List

from .lif_neuron import BioLIFNeuron
from snn_research.learning_rules.base_rule import BioLearningRule
from snn_research.learning_rules.causal_trace import CausalTraceCreditAssignment

class BioSNN(nn.Module):
    """ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã§å­¦ç¿’ã™ã‚‹ã€è¤‡æ•°å±¤ã«å¯¾å¿œã—ãŸSNNãƒ¢ãƒ‡ãƒ«ã€‚"""
    def __init__(self, layer_sizes: List[int], neuron_params: dict, learning_rule: BioLearningRule, 
                 sparsification_config: Optional[Dict[str, Any]] = None):
        super().__init__()
        self.layer_sizes = layer_sizes
        self.learning_rule = learning_rule
        self.sparsification_enabled = sparsification_config.get("enabled", False) if sparsification_config else False
        self.contribution_threshold = sparsification_config.get("contribution_threshold", 0.0) if sparsification_config else 0.0
        if self.sparsification_enabled:
            print(f"ğŸ§¬ é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãŒæœ‰åŠ¹ã§ã™ (è²¢çŒ®åº¦é–¾å€¤: {self.contribution_threshold})")
        
        # å±¤ã¨é‡ã¿ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        self.layers = nn.ModuleList()
        self.weights = nn.ParameterList()
        
        for i in range(len(layer_sizes) - 1):
            self.layers.append(BioLIFNeuron(layer_sizes[i+1], neuron_params))
            # é‡ã¿ã‚’Parameterã¨ã—ã¦ç™»éŒ²
            weight = nn.Parameter(torch.rand(layer_sizes[i+1], layer_sizes[i]) * 0.5)
            self.weights.append(weight)

    def forward(self, input_spikes: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """æ¨è«–ã®ã¿ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚å‡ºåŠ›ã‚¹ãƒ‘ã‚¤ã‚¯ã¨å„å±¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯å±¥æ­´ã‚’è¿”ã™ã€‚"""
        hidden_spikes_history = []
        current_spikes = input_spikes
        
        for i, layer in enumerate(self.layers):
            current = torch.matmul(self.weights[i], current_spikes)
            current_spikes = layer(current)
            hidden_spikes_history.append(current_spikes)
            
        return current_spikes, hidden_spikes_history
        
    def update_weights(
        self,
        all_layer_spikes: List[torch.Tensor],
        optional_params: Optional[Dict[str, Any]] = None
    ):
        """
        å­¦ç¿’å‰‡ã«åŸºã¥ã„ã¦å…¨å±¤ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€‚
        å¾Œæ®µã®å±¤ã‹ã‚‰å‰æ®µã®å±¤ã¸ã€å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’é€†ä¼æ’­ã•ã›ã‚‹ã€‚
        """
        if not self.training:
            return

        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # éšå±¤çš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦ã®ãŸã‚ã€å¾Œã‚ã®å±¤ã‹ã‚‰é †ç•ªã«æ›´æ–°
        backward_credit = None
        current_params = optional_params.copy() if optional_params else {}

        for i in reversed(range(len(self.weights))):
            pre_spikes = all_layer_spikes[i]
            post_spikes = all_layer_spikes[i+1]
            
            # å¾Œæ®µã®å±¤ã‹ã‚‰ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’ç¾åœ¨ã®å ±é…¬ã¨çµåˆã™ã‚‹
            if backward_credit is not None:
                # å¤–éƒ¨å ±é…¬ã¨å†…éƒ¨ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’çµ„ã¿åˆã‚ã›ã‚‹ï¼ˆã“ã“ã§ã¯å˜ç´”ãªåŠ ç®—ï¼‰
                reward_signal = current_params.get("reward", 0.0)
                modulated_reward = reward_signal + backward_credit.mean().item() # ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã«å¤‰æ›
                current_params["reward"] = modulated_reward

            # CausalTraceCreditAssignment.update ã¯ (dw, backward_credit) ã‚’è¿”ã™
            dw, backward_credit = self.learning_rule.update(
                pre_spikes=pre_spikes, 
                post_spikes=post_spikes,
                weights=self.weights[i],
                optional_params=current_params
            )
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

            # é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã®ãƒ­ã‚¸ãƒƒã‚¯
            if self.sparsification_enabled and isinstance(self.learning_rule, CausalTraceCreditAssignment):
                if self.learning_rule.causal_contribution is not None:
                    # è²¢çŒ®åº¦ãŒé–¾å€¤ä»¥ä¸‹ã®ã‚·ãƒŠãƒ—ã‚¹ã®å­¦ç¿’ã‚’æŠ‘åˆ¶ï¼ˆã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰
                    contribution_mask = self.learning_rule.causal_contribution > self.contribution_threshold
                    dw = dw * contribution_mask

            # nn.Parameterã®æ›´æ–°ã¯ .data ã‚’ä½¿ã†
            self.weights[i].data += dw
            # é‡ã¿ãŒè² ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            self.weights[i].data.clamp_(min=0)
