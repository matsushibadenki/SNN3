# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/learning_rules/causal_trace.py
# (æ›´æ–°)
# Title: å› æœè¿½è·¡ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦ (Causal Trace Credit Assignment)
# Description: å¾®åˆ†ã‹ã‚‰è„±å´ã—ã€ã‚¹ãƒ‘ã‚¤ã‚¯ã®å› æœé€£é–ã‚’è¾¿ã£ã¦ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹
#              æ–°ã—ã„ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã€‚
# æ”¹å–„ç‚¹: ã€Œé©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã€ã®ãŸã‚ã€ã‚·ãƒŠãƒ—ã‚¹ã®å› æœçš„è²¢çŒ®åº¦ã‚’è¿½è·¡ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# æ”¹å–„ç‚¹ (v2): éšå±¤çš„å› æœå­¦ç¿’ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã€å¾Œæ®µã®å±¤ã‹ã‚‰å‰æ®µã®å±¤ã¸
#              ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’é€†ä¼æ’­ã•ã›ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚

import torch
from typing import Dict, Any, Optional, Tuple
from .reward_modulated_stdp import RewardModulatedSTDP

class CausalTraceCreditAssignment(RewardModulatedSTDP):
    """
    å› æœè¿½è·¡ã®æ€æƒ³ã«åŸºã¥ãã€é©æ ¼æ€§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç”¨ã„ã¦ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆå ±é…¬ï¼‰ã‚’å‰²ã‚Šå½“ã¦ã‚‹å­¦ç¿’ãƒ«ãƒ¼ãƒ«ã€‚
    RewardModulatedSTDPã‚’ç¶™æ‰¿ã—ã€ãã®è²¬å‹™ã‚’ã‚ˆã‚Šæ˜ç¢ºã«ã™ã‚‹ã€‚
    """
    def __init__(self, learning_rate: float, a_plus: float, a_minus: float, tau_trace: float, tau_eligibility: float, dt: float = 1.0):
        super().__init__(learning_rate, a_plus, a_minus, tau_trace, tau_eligibility, dt)
        self.causal_contribution: Optional[torch.Tensor] = None
        print("ğŸ§  Causal Trace Credit Assignment learning rule initialized.")

    def _initialize_contribution_trace(self, weight_shape: tuple, device: torch.device):
        """å› æœçš„è²¢çŒ®åº¦ã‚’è¨˜éŒ²ã™ã‚‹ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚"""
        self.causal_contribution = torch.zeros(weight_shape, device=device)

    def update(
        self,
        pre_spikes: torch.Tensor,
        post_spikes: torch.Tensor,
        weights: torch.Tensor,
        optional_params: Optional[Dict[str, Any]] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å ±é…¬ä¿¡å·ã¨å¾Œæ®µã‹ã‚‰ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã«åŸºã¥ãé‡ã¿å¤‰åŒ–é‡ã‚’è¨ˆç®—ã—ã€
        å‰æ®µã¸ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’ç”Ÿæˆã—ã¦è¿”ã™ã€‚

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (é‡ã¿å¤‰åŒ–é‡, å‰æ®µã¸ä¼ãˆã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·)
        """
        # RewardModulatedSTDPã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—ã€åŸºæœ¬çš„ãªé‡ã¿å¤‰åŒ–é‡ã‚’å–å¾—
        dw = super().update(pre_spikes, post_spikes, weights, optional_params)

        if self.causal_contribution is None or self.causal_contribution.shape != weights.shape:
            self._initialize_contribution_trace(weights.shape, weights.device)
        
        assert self.causal_contribution is not None, "Causal contribution trace not initialized."

        # å ±é…¬ãŒã‚ã£ãŸå ´åˆã€ãã®æ›´æ–°ã®å¤§ãã•ã‚’è²¢çŒ®åº¦ã¨ã—ã¦è¨˜éŒ²
        if optional_params and optional_params.get("reward", 0.0) != 0.0:
            self.causal_contribution = self.causal_contribution * 0.99 + torch.abs(dw) * 0.01

        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“è¿½åŠ é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # éšå±¤çš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦ã®ãŸã‚ã®é€†æ–¹å‘ä¿¡å·ã‚’è¨ˆç®—
        # post-synapticãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»å‹•ï¼ˆã¾ãŸã¯èª¤å·®ï¼‰ã‚’ã€çµåˆé‡ã¿ã‚’ä»‹ã—ã¦
        # pre-synapticãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã€Œè²¬ä»»ã€ã¨ã—ã¦é€†æŠ•å½±ã™ã‚‹ã€‚
        # ã“ã“ã§ã¯ã€é©æ ¼æ€§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’èª¤å·®ä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã€‚
        if self.eligibility_trace is not None:
            # eligibility_trace: [post, pre], weights: [post, pre]
            # backward_credit: [pre]
            backward_credit = torch.einsum('ij,ij->j', self.eligibility_trace, weights)
        else:
            backward_credit = torch.zeros_like(pre_spikes)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘è¿½åŠ çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

        return dw, backward_credit
