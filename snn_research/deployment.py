# matsushibadenki/snn3/snn_research/deployment.py
# SNNã®å®Ÿç”¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ãŸã‚ã®æœ€é©åŒ–ã€ç›£è¦–ã€ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
#
# å¤‰æ›´ç‚¹:
# - AdaptiveQuantizationPruningã‚¯ãƒ©ã‚¹ã«ã€å…·ä½“çš„ãªãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡å­åŒ–ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã€‚
#   - nn.utils.prune ã‚’åˆ©ç”¨ã—ãŸMagnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å°å…¥ã€‚
#   - torch.quantization ã‚’åˆ©ç”¨ã—ãŸå‹•çš„é‡å­åŒ–ã‚’å°å…¥ã€‚
# - SNNInferenceEngineãŒãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã« strict=False ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
# - mypyã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ã€å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚
# - ç‹¬è‡ªVocabularyã‚’å»ƒæ­¢ã—ã€Hugging Face Tokenizerã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«SNNInferenceEngineã‚’ä¿®æ­£ã€‚
# - `generate` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼‰ã«å¤‰æ›´ã—ã€é€æ¬¡çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å¯èƒ½ã«ã€‚
# - `stop_sequences` ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ”¹å–„ã—ã€ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã«å«ã¾ã‚Œã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# - æ¨è«–æ™‚ã®ç·ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã‚’è¨ˆæ¸¬ã—ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•° `last_inference_stats` ã«ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - [æ”¹å–„] generateãƒ¡ã‚½ãƒƒãƒ‰ã«ã€Top-KãŠã‚ˆã³Top-P (Nucleus)ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’è¿½åŠ ã€‚
# - [ä¿®æ­£] mypyã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ã€_sample_next_tokenã®æˆ»ã‚Šå€¤ã‚’intã«ã‚­ãƒ£ã‚¹ãƒˆã—ã€å‹ãƒ’ãƒ³ãƒˆã‚’ä¿®æ­£ã€‚

import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import os
import copy
import time
from typing import Dict, Any, List, Optional, Iterator
from enum import Enum
from dataclasses import dataclass
from transformers import AutoTokenizer
import torch.nn.functional as F

# --- SNN æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ ---
class SNNInferenceEngine:
    """SNNãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    def __init__(self, model_path: str, device: str):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

        from .core.snn_core import BreakthroughSNN

        self.model_path = model_path
        self.device = torch.device(device)
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if 'config' in checkpoint:
            self.config: Dict[str, Any] = checkpoint['config']
            tokenizer_name = checkpoint.get('tokenizer_name', 'gpt2')
        else:
            print("âš ï¸ å¤ã„å½¢å¼ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.config = {'d_model': 128, 'd_state': 64, 'num_layers': 4, 'time_steps': 20, 'n_head': 2}
            tokenizer_name = 'gpt2'

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = BreakthroughSNN(vocab_size=self.tokenizer.vocab_size, **self.config).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        self.model.eval()
        self.last_inference_stats: Dict[str, Any] = {}

    def _sample_next_token(self, logits: torch.Tensor, top_k: int, top_p: float, temperature: float) -> int:
        """Top-K, Top-Pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç”¨ã„ã¦æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ±ºå®šã™ã‚‹"""
        logits = logits / temperature

        if top_k > 0:
            top_k = min(top_k, logits.size(-1))
            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
            logits[indices_to_remove] = -float("Inf")

        if top_p > 0.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0

            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits[indices_to_remove] = -float("Inf")
        
        probs = F.softmax(logits, dim=-1)
        next_token_id = torch.multinomial(probs, num_samples=1)
        return int(next_token_id.item())

    def generate(self, start_text: str, max_len: int, stop_sequences: Optional[List[str]] = None,
                 top_k: int = 50, top_p: float = 0.95, temperature: float = 0.8) -> Iterator[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§ç”Ÿæˆã—ã¾ã™ã€‚
        """
        self.last_inference_stats = {"total_spikes": 0, "total_mem": 0}

        bos_token = self.tokenizer.bos_token or ''
        prompt_ids = self.tokenizer.encode(f"{bos_token}{start_text}", return_tensors='pt').to(self.device)
        
        input_tensor = prompt_ids
        generated_text = ""
        
        with torch.no_grad():
            for _ in range(max_len):
                logits, spikes, mem = self.model(input_tensor)
                
                if spikes.numel() > 0: self.last_inference_stats["total_spikes"] += spikes.sum().item()
                if mem.numel() > 0: self.last_inference_stats["total_mem"] += mem.abs().sum().item()
                
                next_token_logits = logits[:, -1, :]
                
                next_token_id = self._sample_next_token(next_token_logits, top_k, top_p, temperature)
                
                if next_token_id == self.tokenizer.eos_token_id:
                    break

                new_token = self.tokenizer.decode([next_token_id])
                generated_text += new_token
                yield new_token
                
                if stop_sequences:
                    if any(stop_seq in generated_text for stop_seq in stop_sequences):
                        break
                    
                input_tensor = torch.cat([input_tensor, torch.tensor([[next_token_id]], device=self.device)], dim=1)


# --- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ ---
class NeuromorphicChip(Enum):
    INTEL_LOIHI = "intel_loihi"
    IBM_TRUENORTH = "ibm_truenorth"
    GENERIC_EDGE = "generic_edge"

@dataclass
class NeuromorphicProfile:
    chip_type: NeuromorphicChip
    num_cores: int
    power_budget_mw: float

class AdaptiveQuantizationPruning:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡å­åŒ–ã‚’é©ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚"""
    def apply_pruning(self, model: nn.Module, pruning_ratio: float):
        """
        Magnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒ¢ãƒ‡ãƒ«ã®Linearå±¤ã«é©ç”¨ã™ã‚‹ã€‚
        """
        if pruning_ratio <= 0: return
        print(f"Applying magnitude pruning with ratio: {pruning_ratio}")
        for module in model.modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name="weight", amount=pruning_ratio)
                prune.remove(module, 'weight')
    
    def apply_quantization(self, model: nn.Module, bits: int) -> nn.Module:
        """
        å‹•çš„é‡å­åŒ–ã‚’ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã™ã‚‹ã€‚
        """
        if bits >= 32: return model
        print(f"Applying dynamic quantization to {bits}-bit...")
        model_to_quantize = copy.deepcopy(model).cpu()
        quantized_model = torch.quantization.quantize_dynamic(
            model_to_quantize, {nn.Linear}, dtype=torch.qint8
        )
        return quantized_model

class NeuromorphicDeploymentManager:
    def __init__(self, profile: NeuromorphicProfile):
        self.profile = profile
        self.adaptive_compression = AdaptiveQuantizationPruning()

    def deploy_model(self, model: nn.Module, name: str, optimization_target: str = "balanced"):
        print(f"ğŸ”§ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹: {name}")
        if optimization_target == "balanced": sparsity, bit_width = 0.7, 8
        elif optimization_target == "ultra_low_power": sparsity, bit_width = 0.9, 8
        else: sparsity, bit_width = 0.5, 16
        
        optimized_model = copy.deepcopy(model)
        optimized_model.eval()
        
        print(f"  - ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é©ç”¨ä¸­ (ã‚¹ãƒ‘ãƒ¼ã‚¹ç‡: {sparsity})...")
        self.adaptive_compression.apply_pruning(optimized_model, float(sparsity))
        
        print(f"  - é‡å­åŒ–é©ç”¨ä¸­ (ãƒ“ãƒƒãƒˆå¹…: {bit_width}-bit)...")
        optimized_model = self.adaptive_compression.apply_quantization(optimized_model, int(bit_width))

        print(f"âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Œäº†: {name}")
        return optimized_model