# matsushibadenki/snn3/snn_research/core/snn_core.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã€æ¬¡ä¸–ä»£ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãªã©ã€ä¸­æ ¸ã¨ãªã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’é›†ç´„ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒª
#
# æ ¹æœ¬çš„ãªã‚¨ãƒ©ãƒ¼ä¿®æ­£:
# SpikingTransformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å…¨é¢çš„ã«ä¿®æ­£ã€‚
# RNNçš„ãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å‡¦ç†ã¨Transformerçš„ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹å‡¦ç†ã®æ··åœ¨ãŒã‚¨ãƒ©ãƒ¼ã®æ ¹æœ¬åŽŸå› ã§ã‚ã£ãŸãŸã‚ã€
# Spiking Transformerã®æ¨™æº–çš„ãªå®Ÿè£…ã«ä¿®æ­£ã€‚
# ä¿®æ­£å¾Œã®å‹•ä½œ:
# 1. å¤–éƒ¨ã§`time_steps`ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
# 2. å„æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã„ã¦ã€Attentionãƒ–ãƒ­ãƒƒã‚¯ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“(B, N, C)ã‚’å—ã‘å–ã£ã¦å‡¦ç†ã™ã‚‹ã€‚
# 3. å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ«ãƒ¼ãƒ—ã‚’é€šã˜ã¦å†…éƒ¨çŠ¶æ…‹ï¼ˆè†œé›»ä½ï¼‰ã‚’æ›´æ–°ã™ã‚‹ã€‚
# ã“ã‚Œã«ã‚ˆã‚Šã€ã™ã¹ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã®ä¸æ•´åˆãŒè§£æ¶ˆã•ã‚Œã‚‹ã€‚
# TypeErrorä¿®æ­£:
# nn.Sequentialå†…ã§ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŸã‚ã€TypeErrorãŒç™ºç”Ÿã—ã¦ã„ãŸã€‚
# STAttenBlockå†…ã®FFNã‚’æ‰‹å‹•ã§ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã—ã¦å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# TypeErrorä¿®æ­£ (SimpleSNN):
# SimpleSNNãŒå¤ã„BioLIFNeuronã‚’å¼•æ•°ãªã—ã§å‘¼ã³å‡ºã—ã¦ã„ãŸå•é¡Œã‚’ä¿®æ­£ã€‚
# ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨æ•´åˆæ€§ã‚’å–ã‚‹ãŸã‚ã€AdaptiveLIFNeuronã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
from spikingjelly.activation_based import surrogate, functional  # type: ignore
from typing import Tuple, Dict, Any, Optional, List, Type, cast
import math
from omegaconf import DictConfig, OmegaConf
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
# from snn_research.bio_models.lif_neuron import BioLIFNeuron as LIFNeuron # å¤ã„LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å‰Šé™¤
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸


# --- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ« ---
class AdaptiveLIFNeuron(nn.Module):
    """
    é©å¿œçš„ç™ºç«é–¾å€¤ã‚’æŒã¤LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ (è¡¨ç¾åŠ›å‘ä¸Šã®ãŸã‚ã®æ¨™æº–ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³)
    """
    def __init__(
        self,
        features: int,
        tau: float = 2.0,
        base_threshold: float = 1.0,
        adaptation_strength: float = 0.1,
        target_spike_rate: float = 0.02,
    ):
        super().__init__()
        self.tau = tau
        self.base_threshold = base_threshold
        self.adaptation_strength = adaptation_strength
        self.target_spike_rate = target_spike_rate
        self.surrogate_function = surrogate.ATan()
        self.mem_decay = math.exp(-1.0 / tau)
        self.register_buffer(
            "adaptive_threshold", torch.full((features,), base_threshold)
        )
        self.adaptive_threshold: torch.Tensor
        self.register_buffer("mem", torch.zeros(1, features))
        self.mem: torch.Tensor

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒæ•°ã«å¿œã˜ã¦è†œé›»ä½ãƒãƒƒãƒ•ã‚¡ã®å½¢çŠ¶ã‚’èª¿æ•´
        if self.mem.shape[0] != x.shape[0] or self.mem.device != x.device or self.mem.ndim != x.ndim:
            self.mem = torch.zeros_like(x, device=x.device)

        mem_this_step = self.mem * self.mem_decay + x
        spike = self.surrogate_function(mem_this_step - self.adaptive_threshold)
        
        mem_for_next_step = mem_this_step * (1.0 - spike)
        self.mem = mem_for_next_step.detach()

        if self.training:
            with torch.no_grad():
                spike_rate_error = spike.mean() - self.target_spike_rate
                self.adaptive_threshold += self.adaptation_strength * spike_rate_error
                self.adaptive_threshold.clamp_(min=0.5)

        return spike, mem_this_step

class DendriticNeuron(nn.Module):
    """
    Phase 4: æ¨¹çŠ¶çªèµ·æ¼”ç®—ã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã€‚
    """
    def __init__(self, input_features: int, num_branches: int, branch_features: int):
        super().__init__()
        self.num_branches = num_branches
        self.branches = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_features, branch_features),
                AdaptiveLIFNeuron(branch_features)
            ) for _ in range(num_branches)
        ])
        self.soma_lif = AdaptiveLIFNeuron(branch_features * num_branches)
        self.output_projection = nn.Linear(branch_features * num_branches, input_features)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        branch_outputs = [branch(x)[0] for branch in self.branches]
        concatenated_spikes = torch.cat(branch_outputs, dim=-1)
        soma_spike, soma_mem = self.soma_lif(concatenated_spikes)
        output = self.output_projection(soma_spike)
        return output, soma_mem

class SNNLayerNorm(nn.Module):
    """SNNã®ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¡Œã†LayerNorm"""
    def __init__(self, normalized_shape):
        super().__init__()
        self.norm = nn.LayerNorm(normalized_shape)
    def forward(self, x):
        return self.norm(x)

# --- äºˆæ¸¬ç¬¦å·åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ ---
class PredictiveCodingLayer(nn.Module):
    """äºˆæ¸¬ç¬¦å·åŒ–ã‚’å®Ÿè¡Œã™ã‚‹å˜ä¸€ã®éšŽå±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€‚"""
    def __init__(self, d_model: int, d_state: int, n_head: int, neuron_class: Type[nn.Module], neuron_params: Dict[str, Any]):
        super().__init__()
        self.generative_fc = nn.Linear(d_state, d_model)
        if neuron_class == DendriticNeuron:
            self.generative_neuron = neuron_class(input_features=d_model, **neuron_params)
        else:
            self.generative_neuron = neuron_class(features=d_model)

        self.inference_fc = nn.Linear(d_model, d_state)
        if neuron_class == DendriticNeuron:
            self.inference_neuron = neuron_class(input_features=d_state, **neuron_params)
        else:
            self.inference_neuron = neuron_class(features=d_state)
        
        self.norm_state = SNNLayerNorm(d_state)
        self.norm_error = SNNLayerNorm(d_model)

    def forward(self, bottom_up_input: torch.Tensor, top_down_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        prediction, _ = self.generative_neuron(self.generative_fc(top_down_state))
        prediction_error = F.relu(bottom_up_input - prediction)
        prediction_error = self.norm_error(prediction_error)
        state_update, inference_mem = self.inference_neuron(self.inference_fc(prediction_error))
        updated_state = self.norm_state(top_down_state + state_update)
        return updated_state, prediction_error, prediction, inference_mem

# --- ã‚³ã‚¢SNNãƒ¢ãƒ‡ãƒ« (äºˆæ¸¬ç¬¦å·åŒ–) ---
class BreakthroughSNN(nn.Module):
    """ãƒªã‚«ãƒ¬ãƒ³ãƒˆäºˆæ¸¬ç¬¦å·åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ã—ãŸéšŽå±¤çš„SNN"""
    def __init__(self, vocab_size: int, d_model: int, d_state: int, num_layers: int, time_steps: int, n_head: int, neuron_config: Optional[Dict[str, Any]] = None):
        super().__init__()
        self.time_steps = time_steps
        self.num_layers = num_layers
        self.d_model = d_model
        self.d_state = d_state
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        neuron_config = neuron_config or {"type": "lif"}
        neuron_type = neuron_config.get("type", "lif")
        neuron_class: Type[nn.Module]

        if neuron_type == "dendritic":
            neuron_class = DendriticNeuron
            neuron_params = {
                "num_branches": neuron_config.get("num_branches", 4),
                "branch_features": neuron_config.get("branch_features", d_model // 4)
            }
            self.input_encoder = nn.Sequential(nn.Linear(d_model, d_model), DendriticNeuron(input_features=d_model, **neuron_params))
            print("ðŸ’¡ BreakthroughSNN initialized with Dendritic Neurons.")
        else:
            neuron_class = AdaptiveLIFNeuron
            neuron_params = {}
            self.input_encoder = nn.Sequential(nn.Linear(d_model, d_model), AdaptiveLIFNeuron(features=d_model))
        
        self.pc_layers = nn.ModuleList(
            [PredictiveCodingLayer(d_model, d_state, n_head, neuron_class, neuron_params) for _ in range(num_layers)]
        )
        self.output_projection = nn.Linear(d_model, vocab_size)

    def forward(
        self,
        input_ids: torch.Tensor,
        return_spikes: bool = False,
        return_full_mems: bool = False,
        output_hidden_states: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        batch_size, seq_len = input_ids.shape
        token_emb = self.token_embedding(input_ids)
        states = [torch.zeros(batch_size, self.d_state, device=input_ids.device) for _ in range(self.num_layers)]
        
        total_spikes = torch.tensor(0.0, device=input_ids.device)
        total_mem_potential = torch.tensor(0.0, device=input_ids.device)
        all_hidden_states: List[torch.Tensor] = []
        all_mems_list: List[torch.Tensor] = []

        # spikingjellyã®ä½œæ³•ã«å¾“ã„ã€å‡¦ç†é–‹å§‹å‰ã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        functional.reset_net(self)

        for i in range(seq_len):
            embedded_token = token_emb[:, i, :]
            bottom_up_input, _ = self.input_encoder(embedded_token)
            
            layer_errors: List[torch.Tensor] = []
            layer_mems: List[torch.Tensor] = []
            
            for j in range(self.num_layers):
                states[j], error, _, mem = self.pc_layers[j](bottom_up_input, states[j])
                bottom_up_input = error
                layer_errors.append(error)
                layer_mems.append(mem)
            
            top_most_state = states[-1]
            _, _, final_prediction, _ = self.pc_layers[-1](
                torch.zeros_like(bottom_up_input, device=input_ids.device),
                top_most_state
            )
            all_hidden_states.append(final_prediction)

            if return_spikes or return_full_mems:
                total_spikes += sum(err.mean() for err in layer_errors)
                current_mem_avg = sum((m.abs().mean() for m in layer_mems), start=torch.tensor(0.0, device=input_ids.device))
                total_mem_potential += current_mem_avg
                if return_full_mems:
                    all_mems_list.append(current_mem_avg)

        final_hidden_states = torch.stack(all_hidden_states, dim=1)
        
        if output_hidden_states:
            final_output = final_hidden_states
        else:
            final_output = self.output_projection(final_hidden_states)

        avg_spikes = total_spikes / seq_len if return_spikes and seq_len > 0 else torch.tensor(0.0)
        final_mem = torch.stack(all_mems_list) if return_full_mems and all_mems_list else total_mem_potential / seq_len if seq_len > 0 else torch.tensor(0.0)

        return final_output, avg_spikes, final_mem

# --- â—¾ï¸â—¾ï¸â—¾ï¸â†“Spiking Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆ)â†“â—¾ï¸â—¾ï¸â—¾ï¸ ---
class SpikeDrivenSelfAttention(nn.Module):
    """ã‚¹ãƒ‘ã‚¤ã‚¯é§†å‹•ã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã€‚"""
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        self.d_model = d_model
        self.n_head = n_head
        self.d_head = d_model // n_head
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        self.neuron_q = AdaptiveLIFNeuron(features=d_model)
        self.neuron_k = AdaptiveLIFNeuron(features=d_model)
        self.neuron_out = AdaptiveLIFNeuron(features=d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, C = x.shape
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        q_spike, _ = self.neuron_q(q)
        k_spike, _ = self.neuron_k(k)

        q_spike_mha = q_spike.view(B, N, self.n_head, self.d_head).permute(0, 2, 1, 3)
        k_spike_mha = k_spike.view(B, N, self.n_head, self.d_head).permute(0, 2, 3, 1)
        v_mha = v.view(B, N, self.n_head, self.d_head).permute(0, 2, 1, 3)

        attn_scores = torch.matmul(q_spike_mha, k_spike_mha) / math.sqrt(self.d_head)
        attn_weights_spike = torch.sigmoid(attn_scores)

        attn_output = torch.matmul(attn_weights_spike, v_mha)
        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, N, C)
        
        output = self.out_proj(attn_output)
        output_spike, _ = self.neuron_out(output)
        return output_spike

class STAttenBlock(nn.Module):
    """ç©ºé–“æ™‚é–“ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã€‚"""
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        self.norm1 = SNNLayerNorm(d_model)
        self.attn = SpikeDrivenSelfAttention(d_model, n_head)
        self.norm2 = SNNLayerNorm(d_model)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # FFNã‚’åˆ†è§£ã—ã¦ã€LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¿ãƒ—ãƒ«å‡ºåŠ›ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
        self.fc1 = nn.Linear(d_model, d_model * 4)
        self.lif1 = AdaptiveLIFNeuron(features=d_model * 4)
        self.fc2 = nn.Linear(d_model * 4, d_model)
        self.lif2 = AdaptiveLIFNeuron(features=d_model)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x shape: (Batch, Sequence, Channels)
        x = x + self.attn(self.norm1(x))
        
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # FFNã‚’æ‰‹å‹•ã§å®Ÿè¡Œã—ã€ã‚¿ãƒ—ãƒ«ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹
        identity = x
        out = self.norm2(x)
        out = self.fc1(out)
        out_spike, _ = self.lif1(out) # ã‚¿ãƒ—ãƒ«ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
        out = self.fc2(out_spike)
        out_spike, _ = self.lif2(out) # ã‚¿ãƒ—ãƒ«ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
        
        x = identity + out_spike
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        return x

class SpikingTransformer(nn.Module):
    """æ™‚é–“ä¾¡å€¤ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€ç©ºé–“æ™‚é–“ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å‚™ãˆãŸSpiking Transformerã€‚"""
    def __init__(self, vocab_size: int, d_model: int, n_head: int, num_layers: int, time_steps: int, **kwargs: Any):
        super().__init__()
        self.d_model = d_model
        self.time_steps = time_steps
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Parameter(torch.randn(1, 1024, d_model)) # ååˆ†ã«å¤§ãã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’ç¢ºä¿
        
        self.layers = nn.ModuleList([STAttenBlock(d_model, n_head) for _ in range(num_layers)])
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        print(f"ðŸš€ Spiking Transformer (STAtten) initialized with {num_layers} layers.")

    def forward(self, input_ids: torch.Tensor, return_spikes: bool = False, return_full_mems: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        batch_size, seq_len = input_ids.shape
        x_embed = self.token_embedding(input_ids)
        x_embed = x_embed + self.pos_embedding[:, :seq_len, :]

        functional.reset_net(self)

        outputs_over_time = []
        spike_outputs_over_time = []

        for t in range(self.time_steps):
            x_t = x_embed
            for layer in self.layers:
                x_t = layer(x_t)
            
            # æœ€å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ï¼‰ã‚’è¨˜éŒ²
            spike_outputs_over_time.append(x_t)
            # æœ€çµ‚çš„ãªãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—ã®ãŸã‚ã€ã‚¹ãƒ‘ã‚¤ã‚¯ã§ã¯ãªã„å€¤ï¼ˆè†œé›»ä½ãªã©ï¼‰ã‚‚ä¿æŒã—ãŸã„ãŒã€
            # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’ç©åˆ†ï¼ˆåˆè¨ˆï¼‰ã™ã‚‹
            if len(outputs_over_time) > 0:
                outputs_over_time.append(outputs_over_time[-1] + x_t)
            else:
                outputs_over_time.append(x_t)
        
        # æ™‚é–“æ–¹å‘ã«ç©åˆ†ï¼ˆåˆè¨ˆï¼‰ã—ãŸå€¤ã‚’æœ€çµ‚å‡ºåŠ›ã¨ã™ã‚‹
        final_output = outputs_over_time[-1]

        logits = self.output_projection(final_output)

        total_spikes = torch.tensor(0.0, device=x_embed.device)
        if return_spikes:
             # å…¨æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã€å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’åˆè¨ˆ
             for spikes in spike_outputs_over_time:
                 total_spikes += spikes.sum()
        
        # ãƒãƒƒãƒã¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§å¹³å‡åŒ–
        avg_spikes = total_spikes / (self.time_steps * batch_size * seq_len) if return_spikes else torch.tensor(0.0)
        avg_mems = torch.tensor(0.0, device=x_embed.device)  # Placeholder

        return logits, avg_spikes, avg_mems

# --- â—¾ï¸â—¾ï¸â—¾ï¸â†‘Spiking Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£â†‘â—¾ï¸â—¾ï¸â—¾ï¸ ---

class SimpleSNN(nn.Module):
    """
    åŸºæœ¬çš„ãªLIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§æ§‹æˆã•ã‚ŒãŸã‚·ãƒ³ãƒ—ãƒ«ãªSNNãƒ¢ãƒ‡ãƒ«ã€‚
    """
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleSNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        self.lif1 = AdaptiveLIFNeuron(features=hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.lif2 = AdaptiveLIFNeuron(features=output_size)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(0)
        
        T, B, _ = x.shape
        functional.reset_net(self)
        
        outputs = []
        for t in range(T):
            x_t = x[t, :, :]
            out = self.fc1(x_t)
            out, _ = self.lif1(out)
            out = self.fc2(out)
            out, _ = self.lif2(out)
            outputs.append(out)
            
        return torch.stack(outputs, dim=0)

class SNNCore(nn.Module):
    """
    è¨­å®šã«å¿œã˜ã¦é©åˆ‡ãªSNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self, config: DictConfig, vocab_size: int):
        super(SNNCore, self).__init__()
        if isinstance(config, dict):
            config = OmegaConf.create(config)
        self.config = config
        # SNNCoreã«æ¸¡ã•ã‚Œã‚‹configã¯æ—¢ã«ãƒ¢ãƒ‡ãƒ«è¨­å®šãã®ã‚‚ã®ã§ã‚ã‚‹ãŸã‚ã€.modelã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šé™¤
        model_type = self.config.get("architecture_type", self.config.get("type", "simple"))

        self.model: nn.Module

        # .modelã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šé™¤
        params_untyped = OmegaConf.to_container(self.config, resolve=True)
        if not isinstance(params_untyped, Dict):
            raise ValueError(f"Model configuration must be a dictionary. Got: {type(params_untyped)}")
        
        params: Dict[str, Any] = cast(Dict[str, Any], params_untyped)

        if model_type == "predictive_coding":
            self.model = BreakthroughSNN(
                vocab_size=vocab_size,
                d_model=params.get("d_model", 256),
                d_state=params.get("d_state", 128),
                num_layers=params.get("num_layers", 4),
                time_steps=params.get("time_steps", 20),
                n_head=params.get("n_head", 4),
                neuron_config=params.get("neuron", {})
            )
        elif model_type == "spiking_transformer":
            self.model = SpikingTransformer(
                vocab_size=vocab_size,
                d_model=params.get("d_model", 512),
                n_head=params.get("n_head", 8),
                num_layers=params.get("num_layers", 12),
                time_steps=params.get("time_steps", 32)
            )
        elif model_type == "simple":
            self.model = SimpleSNN(
                input_size=params.get("input_size", 10),
                hidden_size=params.get("hidden_size", 50),
                output_size=params.get("output_size", 10)
            )
        else:
            raise ValueError(f"Unknown model type: {model_type}")

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        return self.model(*args, **kwargs)
