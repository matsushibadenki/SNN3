# matsushibadenki/snn3/run_rl_agent.py
# Title: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã«åŸºã¥ãSNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’èµ·å‹•ã—ã€
#              å¼·åŒ–å­¦ç¿’ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2æ¤œè¨¼ã®ãŸã‚ã€GridWorldEnvã«å¯¾å¿œã€‚
# - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚

import torch
import argparse

from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.rl_env.grid_world import GridWorldEnv
from tqdm import tqdm  # type: ignore

def main():
    parser = argparse.ArgumentParser(description="Biologically Plausible Reinforcement Learning Framework")
    parser.add_argument("--episodes", type=int, default=500, help="Number of learning episodes.")
    parser.add_argument("--grid_size", type=int, default=5, help="Size of the grid world.")
    parser.add_argument("--max_steps", type=int, default=50, help="Maximum steps per episode.")
    args = parser.parse_args()

    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    env = GridWorldEnv(size=args.grid_size, max_steps=args.max_steps, device=device)
    # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã¯4æ¬¡å…ƒ(agent_x, agent_y, goal_x, goal_y)ã€è¡Œå‹•ã¯4æ¬¡å…ƒ(ä¸Š/ä¸‹/å·¦/å³)
    agent = ReinforcementLearnerAgent(input_size=4, output_size=4, device=device)

    print("\n" + "="*20 + "ğŸ¤– ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’é–‹å§‹ (Grid World) ğŸ¤–" + "="*20)
    
    progress_bar = tqdm(range(args.episodes))
    total_rewards = []

    for episode in progress_bar:
        state = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            # 1. è¡Œå‹•é¸æŠ
            action = agent.get_action(state)
            
            # 2. ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨
            next_state, reward, done = env.step(action)
            
            # 3. å­¦ç¿’ï¼ˆæ¯ã‚¹ãƒ†ãƒƒãƒ—å ±é…¬ã‚’æ¸¡ã™ãŒã€å®Ÿéš›ã®é‡ã¿æ›´æ–°ã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã«è¡Œã‚ã‚Œã‚‹ï¼‰
            agent.learn(reward)
            
            episode_reward += reward
            state = next_state
        
        total_rewards.append(episode_reward)
        # æœ€è¿‘10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡å ±é…¬ã‚’è¨ˆç®—
        avg_reward = sum(total_rewards[-10:]) / len(total_rewards[-10:])
        
        progress_bar.set_description(f"Episode {episode+1}/{args.episodes}")
        progress_bar.set_postfix({"Last Reward": f"{episode_reward:.2f}", "Avg Reward (last 10)": f"{avg_reward:.3f}"})

    final_avg_reward = sum(total_rewards) / args.episodes if args.episodes > 0 else 0
    print("\n" + "="*20 + "âœ… å­¦ç¿’å®Œäº†" + "="*20)
    print(f"æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {final_avg_reward:.4f}")


if __name__ == "__main__":
    main()
